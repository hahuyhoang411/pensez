{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74af2b68-b077-423e-834f-7aeb357d85e3",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb82af-309d-4435-990b-cccb368cbefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import flash_attn\n",
    "import time\n",
    "\n",
    "model_path = \"HoangHa/Pensez-v0.1-e5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Faudrait demander combien de s il y a dans saucisson\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=16384, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=False, clean_up_tokenization_space=False)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efb553-f2c0-4504-be39-1471122bcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=16384, temperature=0.6, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=False, clean_up_tokenization_space=False)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83cf83-c10e-4553-b03f-bec8ead8876f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"la cyclohexanone est traitée avec du LDA à basse température, suivie de benzaldéhyde. le mélange réactionnel est ensuite acidifié, formant le produit 1. 1 est ensuite traité avec un excès de trifluorure de diéthylaminosoufre, formant le produit 2. quelle est la structure du produit 2 ?\\nA.((R)-((R)-2,2-difluorocyclohexyl)fluorométhyl)benzène\\nB. (2R)-1-fluoro-2-((S)-fluoro(phényl)méthyl)cyclohexan-1-ol\\nC. (S)-2-((R)-fluoro(phényl)méthyl)cyclohexan-1-one\\nD. « ((S)-((R)-2,2-difluorocyclohexyl)fluorométhyl)benzène »\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=32000, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=False, clean_up_tokenization_space=False)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61169428-deb8-4a62-84c2-34b1a0236f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path =  \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ea6d2-5221-46ad-8de5-64d7785e72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system =  \"\"\"Vous êtes un expert en linguistique et en traduction avec de nombreuses années d’expérience.\n",
    "Votre mission est d’analyser en profondeur un texte source avant de le traduire en [français]. L’objectif est d’assurer une traduction précise, contextuellement appropriée, tout en conservant le sens et le style du texte original. Suivez ces étapes :\n",
    "\t1.\tIdentifier le sujet et le sens principal :\n",
    "\t•\tRésumez brièvement le contenu essentiel du texte de manière claire et compréhensible.\n",
    "\t2.\tAnalyser le public cible et le contexte :\n",
    "\t•\tDéterminez à qui s’adresse le texte (ex. : experts, étudiants, consommateurs).\n",
    "\t•\tÉvaluez le contexte d’utilisation (ex. : académique, marketing, personnel).\n",
    "\t3.\tAnalyser le style, le ton et l’émotion :\n",
    "\t•\tIdentifiez le registre du texte (ex. : formel, créatif, technique).\n",
    "\t•\tDécrivez le ton et les émotions véhiculées (ex. : joyeux, sérieux, urgent) et leur impact sur le sens.\n",
    "\t4.\tExaminer le vocabulaire et les expressions spécifiques :\n",
    "\t•\tListez les mots ou expressions clés et expliquez leur signification dans le contexte.\n",
    "\t•\tProposez des équivalents en français qui respectent le contexte et le style du texte.\n",
    "\t5.\tGérer les éléments spécifiques :\n",
    "\t•\tNotez la manière d’aborder les termes techniques, les structures complexes ou les tournures particulières.\n",
    "\t•\tSi le texte est trop complexe, suggérez une reformulation plus simple tout en préservant le sens.\n",
    "\t6.\tAnticiper les défis et proposer des solutions :\n",
    "\t•\tIdentifiez les difficultés potentielles de traduction (ex. : différences culturelles, perte de sens figuré).\n",
    "\t•\tSuggérez des stratégies pour surmonter ces défis.\n",
    "\t7.\tÉvaluer la cohérence et la qualité :\n",
    "\t•\tVérifiez la cohérence terminologique, le maintien des idées et du style dans la traduction.\n",
    "\t•\tDéfinissez des critères pour garantir une traduction fidèle en termes de sens, de style et de contexte.\n",
    "\n",
    "# Version anglaise:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Imagine a radioactive nuclei X(Z,A) can decay into Y(Z-2, A-4) by emitting an alpha particle with partial half life 3.0 minutes. X(Z,A) can also decay into Q(Z+1,A) by decaying a $\\beta^-$ with partial half life 0.098 minutes. If the initial number of X nuclei were 5*10^34 then what is the activity of $\\alpha$ decay after 10 minutes? Note, here Z is proton number and A is mass number. Answer Choices: (A) 1.911*10^31 Bq (B) 3.719 Bq (C) 113.837 Bq (D) 117.555 Bq\"\n",
    "    }\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=2500, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "print(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=False, clean_up_tokenization_space=False)\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f25f5-8f17-484d-ba8b-2c3ca9ac1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess non reasoning fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea279a-3ab3-4c7b-ac25-1c0e9dfb4eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_mt = load_dataset('Magpie-Align/Magpie-Llama-3.1-Pro-MT-300K-Filtered')\n",
    "\n",
    "# Filter for French ('FR') data only\n",
    "dataset_mt = dataset_mt.filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "\n",
    "print(dataset_mt)\n",
    "dataset_mt['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d048ae-8273-4838-8d8a-859a59adb5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_single = load_dataset('Magpie-Align/Magpie-Qwen2.5-Pro-300K-Filtered')\n",
    "\n",
    "# Filter for French ('FR') data only\n",
    "dataset_single = dataset_single.filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "\n",
    "print(dataset_single)\n",
    "dataset_single['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54cf90-60c2-4353-b3eb-56fa94908ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_single_llama = load_dataset('Magpie-Align/Magpie-Llama-3.1-Pro-300K-Filtered')\n",
    "\n",
    "# Filter for French ('FR') data only\n",
    "dataset_single_llama = dataset_single_llama.filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "\n",
    "print(dataset_single_llama)\n",
    "dataset_single_llama['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8284478-60fd-4c47-9113-b17a030db4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_french_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['conversations']\n",
    "    gpt_responses = [conv['value'] for conv in conversations if conv['from'] == 'gpt']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_french = predicted_lang == '__label__fra_Latn' and confidence > 0.9\n",
    "    \n",
    "    return is_french\n",
    "\n",
    "# Apply the French language filter\n",
    "truly_french_dataset = dataset_single.filter(is_french_response)\n",
    "print(f\"Dataset after fasttext language verification: {truly_french_dataset}\")\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset_mt['train'])\n",
    "filtered_count = len(truly_french_dataset['train'])\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original French-labeled examples: {original_count}\")\n",
    "print(f\"Actually French examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")\n",
    "\n",
    "# Optional: Look at some of the removed examples\n",
    "def get_removed_examples():\n",
    "    removed_indices = []\n",
    "    for i, example in enumerate(dataset_mt['train']):\n",
    "        if not is_french_response(example):\n",
    "            removed_indices.append(i)\n",
    "    \n",
    "    return [dataset_mt['train'][i] for i in removed_indices[:5]]  # Return first 5 removed examples\n",
    "\n",
    "removed_examples = get_removed_examples()\n",
    "print(\"\\nExamples of conversations incorrectly labeled as French:\")\n",
    "for i, example in enumerate(removed_examples):\n",
    "    gpt_response = next((conv['value'] for conv in example['conversations'] if conv['from'] == 'gpt'), \"\")\n",
    "    # Remove newlines before prediction\n",
    "    clean_text = gpt_response[:500].replace('\\n', ' ')\n",
    "    prediction = model.predict(clean_text)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Human: {next((conv['value'] for conv in example['conversations'] if conv['from'] == 'human'), '')}\")\n",
    "    print(f\"GPT (first 100 chars): {gpt_response[:100]}...\")\n",
    "    print(f\"Detected language: {prediction[0][0]} with confidence {prediction[1][0]:.4f}\")\n",
    "\n",
    "# Save the filtered dataset\n",
    "# truly_french_dataset.save_to_disk(\"magpie_truly_french_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6506a-81a2-460f-a960-35945656d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fast text\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "# model.predict(\"Torrontés is a white grape variety native to Argentina, and its unique flavor profile is attributed to several factors. Here are some of the key reasons\")\n",
    "\n",
    "model.predict(\"Hello how are you? I'm Rex welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb19634-9b04-4ddc-a97a-448fc7e16d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check lang\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_tulu = load_dataset('allenai/tulu-3-sft-olmo-2-mixture')\n",
    "\n",
    "print(dataset_tulu)\n",
    "dataset_tulu['train'][0]\n",
    "\n",
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_french_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['content'] for conv in conversations if conv['role'] == 'user']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_french = predicted_lang == '__label__fra_Latn' and confidence > 0.95\n",
    "    \n",
    "    return is_french\n",
    "\n",
    "# Apply the French language filter\n",
    "truly_french_dataset = dataset_tulu.filter(is_french_response, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_french_dataset}\")\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset_tulu['train'])\n",
    "filtered_count = len(truly_french_dataset['train'])\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original French-labeled examples: {original_count}\")\n",
    "print(f\"Actually French examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc8eb0-dc71-4084-bb44-97b403ab2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter multiturn\n",
    "import random\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Filter to create multiturn and singleturn datasets\n",
    "multiturn_samples = []\n",
    "singleturn_samples = []\n",
    "\n",
    "for sample in truly_french_dataset['train']:\n",
    "    # Check number of messages to determine if multiturn or singleturn\n",
    "    if len(sample['messages']) > 2:\n",
    "        multiturn_samples.append(sample)\n",
    "    elif len(sample['messages']) == 2:\n",
    "        singleturn_samples.append(sample)\n",
    "\n",
    "# Create datasets from the filtered samples\n",
    "multiturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in multiturn_samples],\n",
    "    'messages': [sample['messages'] for sample in multiturn_samples],\n",
    "    'source': [sample['source'] for sample in multiturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in multiturn_samples]\n",
    "})\n",
    "\n",
    "singleturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in singleturn_samples],\n",
    "    'messages': [sample['messages'] for sample in singleturn_samples],\n",
    "    'source': [sample['source'] for sample in singleturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in singleturn_samples]\n",
    "})\n",
    "\n",
    "# Create and print the dataset dictionaries\n",
    "multiturn_dataset_dict = DatasetDict({'train': multiturn_dataset})\n",
    "singleturn_dataset_dict = DatasetDict({'train': singleturn_dataset})\n",
    "\n",
    "print(\"Multiturn Dataset:\")\n",
    "print(multiturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(multiturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "print(\"\\nSingleturn Dataset:\")\n",
    "print(singleturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(singleturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "# Verify distribution\n",
    "multiturn_count = len(multiturn_dataset)\n",
    "singleturn_count = len(singleturn_dataset)\n",
    "total_count = len(truly_french_dataset['train'])\n",
    "\n",
    "print(f\"\\nDistribution Summary:\")\n",
    "print(f\"Total samples: {total_count}\")\n",
    "print(f\"Multiturn samples: {multiturn_count} ({multiturn_count/total_count:.2%})\")\n",
    "print(f\"Singleturn samples: {singleturn_count} ({singleturn_count/total_count:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789aec4d-5fb7-4a41-b548-544e9af1ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HoangHa/Pensez-Llama3.1-8B\")\n",
    "\n",
    "def count_tokens(example, tokenizer):\n",
    "    \"\"\"Count tokens in messages that have a content field\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in example['messages']:\n",
    "        tokens = tokenizer(message['content'], return_tensors=\"pt\", truncation=False)\n",
    "        total_tokens += len(tokens['input_ids'][0])\n",
    "    example['token_count'] = total_tokens\n",
    "    return example\n",
    "    \n",
    "singleturn_dataset = singleturn_dataset.map(\n",
    "    lambda example: count_tokens(example, tokenizer), \n",
    "    num_proc=16\n",
    ")\n",
    "filterd_singleturn_dataset = singleturn_dataset.filter(lambda x:  512< x['token_count'] < 8000, num_proc=32)\n",
    "filterd_singleturn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1854e9-8525-4696-a8c2-daf2617876ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier analysis\n",
    "df = filterd_singleturn_dataset.to_pandas()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Token Count Statistics:\")\n",
    "print(\"-\" * 22)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Average token count: {df['token_count'].mean():.2f}\")\n",
    "print(f\"Median token count: {df['token_count'].median()}\")\n",
    "print(f\"Minimum token count: {df['token_count'].min()}\")\n",
    "print(f\"Maximum token count: {df['token_count'].max()}\")\n",
    "print(f\"90th percentile: {df['token_count'].quantile(0.9):.0f}\")\n",
    "print(f\"95th percentile: {df['token_count'].quantile(0.95):.0f}\")\n",
    "print(f\"99th percentile: {df['token_count'].quantile(0.99):.0f}\")\n",
    "\n",
    "# Get the 10 longest messages\n",
    "top10 = df.sort_values('token_count', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Longest Messages by Token Count:\")\n",
    "print(\"-\" * 38)\n",
    "for i, (idx, row) in enumerate(top10.iterrows()):\n",
    "    message_preview = row['messages']\n",
    "    if isinstance(message_preview, str) and len(message_preview) > 50:\n",
    "        message_preview = message_preview[:50] + \"...\"\n",
    "    elif isinstance(message_preview, list) and len(str(message_preview)) > 50:\n",
    "        message_preview = str(message_preview)[:50] + \"...\"\n",
    "        \n",
    "    print(f\"{i+1}. ID: {idx}, Token Count: {row['token_count']}\")\n",
    "    print(f\"   Preview: {message_preview}\")\n",
    "\n",
    "# Create a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['token_count'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Token Counts')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Number of Messages')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.axvline(df['token_count'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(df['token_count'].median(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.savefig('token_distribution.png')  # Optional: save the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87056abf-3333-4dd2-a1ba-2390875e29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_singleturn_dataset_filter = filterd_singleturn_dataset.filter(lambda x:  512 < x['token_count'] < 3000, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43520517-4272-4c07-b499-aa68a18eb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_singleturn_dataset_filter = filterd_singleturn_dataset_filter.filter(\n",
    "    lambda x: 'math' not in x['source'].lower() and 'flan' not in x['source'].lower(),\n",
    "    num_proc=16\n",
    ")\n",
    "filterd_singleturn_dataset_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5815713-a4d2-4a3c-9682-88b6b016124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_singleturn_dataset_random = filterd_singleturn_dataset_filter.shuffle(seed=42).select(range(140))\n",
    "filterd_singleturn_dataset_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46032de7-aede-4cd0-9da2-e1fc7b27abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter longest\n",
    "dataset_dicts = [example for example in filterd_singleturn_dataset]\n",
    "\n",
    "# Sort by token count in descending order\n",
    "sorted_dataset = sorted(dataset_dicts, key=lambda x: x['token_count'], reverse=True)\n",
    "\n",
    "# Get the 25 longest samples\n",
    "longest_25 = sorted_dataset[:25]\n",
    "# Create a new dataset with just these 25 samples\n",
    "longest_dataset = Dataset.from_dict({\n",
    "    key: [example[key] for example in longest_25]\n",
    "    for key in longest_25[0].keys()\n",
    "})\n",
    "longest_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2da870-d13e-4e02-9606-f148d25752ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "longest_dataset.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dba03-19b5-45e4-a1b9-8a01cde2d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter task\n",
    "from collections import Counter\n",
    "\n",
    "task_category_counts = Counter(truly_french_dataset['train']['task_category'])\n",
    "print(task_category_counts)\n",
    "truly_french_dataset = truly_french_dataset.filter(lambda x: x['task_category'] != 'Math', num_proc=16)\n",
    "truly_french_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f99562-5633-4bd8-905e-aada43300ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter question\n",
    "def ends_with_question_mark(example):\n",
    "    \"\"\"\n",
    "    Check if the instruction ends with a question mark.\n",
    "    Returns True if it does, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if 'instruction' field exists and has a value\n",
    "    if not example.get('instruction'):\n",
    "        # If not available, check the human message in conversations\n",
    "        conversations = example['conversations']\n",
    "        human_messages = [conv['value'] for conv in conversations if conv['from'] == 'human']\n",
    "        if not human_messages:\n",
    "            return False\n",
    "        instruction = human_messages[0]\n",
    "    else:\n",
    "        instruction = example['instruction']\n",
    "    \n",
    "    # Strip whitespace and check if it ends with a question mark\n",
    "    return instruction.strip().endswith('?')\n",
    "\n",
    "truly_french_dataset = truly_french_dataset.filter(\n",
    "    lambda x: ends_with_question_mark(x)\n",
    ")\n",
    "\n",
    "print(f\"Dataset after language and question mark filters: {truly_french_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0a7c7-3534-4419-9529-15d59ee4cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "truly_french_dataset.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-qwen-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7038c2-0eff-44c2-b3e0-37f0553fbd9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset_cot = load_dataset('Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B')\n",
    "\n",
    "# # Filter for French ('FR') data only\n",
    "# dataset_cot = dataset_cot.filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "\n",
    "# print(dataset_cot)\n",
    "# dataset_cot['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34234-b8c3-46f8-90ab-d98e7cfceeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load and filter each dataset\n",
    "dataset_mt = load_dataset('Magpie-Align/Magpie-Llama-3.1-Pro-MT-300K-Filtered',split='train').filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "dataset_single = load_dataset('Magpie-Align/Magpie-Qwen2.5-Pro-300K-Filtered',split='train').filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "dataset_en = load_dataset('Magpie-Align/Magpie-Qwen2.5-Pro-300K-Filtered',split='train').filter(lambda x: x['language'] == 'EN', num_proc=16).shuffle(seed=42).select(range(200))\n",
    "dataset_single_llama = load_dataset('Magpie-Align/Magpie-Llama-3.1-Pro-300K-Filtered',split='train').filter(lambda x: x['language'] == 'FR', num_proc=16)\n",
    "\n",
    "# Merge the datasets\n",
    "merged_dataset = concatenate_datasets([dataset_mt, dataset_single, dataset_single_llama, dataset_en])\n",
    "\n",
    "print(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267fcbe-f037-4491-b7ad-ad6415c11e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub(\"HoangHa/en-reasoning-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6d3b9-bf00-47d9-9d08-eb63485d97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lang\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_tulu = load_dataset('allenai/tulu-3-sft-olmo-2-mixture')\n",
    "print(dataset_tulu)\n",
    "\n",
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_en_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['content'] for conv in conversations if conv['role'] == 'user']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.95\n",
    "    \n",
    "    return is_eng\n",
    "\n",
    "# Apply the French language filter\n",
    "truly_en_dataset = dataset_tulu.filter(is_en_response, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset_tulu['train'])\n",
    "filtered_count = len(truly_en_dataset['train'])\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original English-labeled examples: {original_count}\")\n",
    "print(f\"Actually English examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aaeba1-0208-46ae-8dc9-9d4e0024a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HoangHa/Pensez-Llama3.1-8B\")\n",
    "\n",
    "def count_tokens(example, tokenizer):\n",
    "    \"\"\"Count tokens in messages that have a content field\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in example['messages']:\n",
    "        tokens = tokenizer(message['content'], return_tensors=\"pt\", truncation=False)\n",
    "        total_tokens += len(tokens['input_ids'][0])\n",
    "    example['token_count'] = total_tokens\n",
    "    return example\n",
    "    \n",
    "truly_en_dataset = truly_en_dataset.map(\n",
    "    lambda example: count_tokens(example, tokenizer), \n",
    "    num_proc=16\n",
    ")\n",
    "filterd_dataset = truly_en_dataset.filter(lambda x:  1024< x['token_count'] < 8192, num_proc=32)\n",
    "filterd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eee7e4-a9c5-48fa-8358-0d81c07716a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter multiturn\n",
    "import random\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Filter to create multiturn and singleturn datasets\n",
    "multiturn_samples = []\n",
    "singleturn_samples = []\n",
    "\n",
    "for sample in filterd_dataset['train']:\n",
    "    # Check number of messages to determine if multiturn or singleturn\n",
    "    if len(sample['messages']) > 2:\n",
    "        multiturn_samples.append(sample)\n",
    "    elif len(sample['messages']) == 2:\n",
    "        singleturn_samples.append(sample)\n",
    "\n",
    "# Create datasets from the filtered samples\n",
    "multiturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in multiturn_samples],\n",
    "    'messages': [sample['messages'] for sample in multiturn_samples],\n",
    "    'source': [sample['source'] for sample in multiturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in multiturn_samples],\n",
    "    'token_count': [sample['token_count'] for sample in multiturn_samples],\n",
    "})\n",
    "\n",
    "singleturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in singleturn_samples],\n",
    "    'messages': [sample['messages'] for sample in singleturn_samples],\n",
    "    'source': [sample['source'] for sample in singleturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in singleturn_samples],\n",
    "    'token_count': [sample['token_count'] for sample in singleturn_samples]\n",
    "})\n",
    "\n",
    "# Create and print the dataset dictionaries\n",
    "multiturn_dataset_dict = DatasetDict({'train': multiturn_dataset})\n",
    "singleturn_dataset_dict = DatasetDict({'train': singleturn_dataset})\n",
    "\n",
    "print(\"Multiturn Dataset:\")\n",
    "print(multiturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(multiturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "print(\"\\nSingleturn Dataset:\")\n",
    "print(singleturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(singleturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "# Verify distribution\n",
    "multiturn_count = len(multiturn_dataset)\n",
    "singleturn_count = len(singleturn_dataset)\n",
    "total_count = len(filterd_dataset['train'])\n",
    "\n",
    "print(f\"\\nDistribution Summary:\")\n",
    "print(f\"Total samples: {total_count}\")\n",
    "print(f\"Multiturn samples: {multiturn_count} ({multiturn_count/total_count:.2%})\")\n",
    "print(f\"Singleturn samples: {singleturn_count} ({singleturn_count/total_count:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb2943-abf8-4f11-8ef7-f04cb0418869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiturn_dataset_random= multiturn_dataset_dict['train'].shuffle(seed=42).select(range(91))\n",
    "multiturn_dataset_random.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee170a-5c13-4eb4-999a-0c5896b0d050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install fasttext huggingface_hub datasets transformers torch\n",
    "\n",
    "# Load fast text\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "# model.predict(\"Torrontés is a white grape variety native to Argentina, and its unique flavor profile is attributed to several factors. Here are some of the key reasons\")\n",
    "\n",
    "model.predict(\"Hello how are you? I'm Rex welcome\")\n",
    "\n",
    "# Check lang\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_tulu = load_dataset('allenai/tulu-3-sft-olmo-2-mixture')\n",
    "print(dataset_tulu)\n",
    "\n",
    "\n",
    "dataset_tulu['train'][0]['messages']\n",
    "\n",
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_en_input(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['content'] for conv in conversations if conv['role'] == 'user']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "\n",
    "def is_en_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['content'] for conv in conversations if conv['role'] == 'assistant']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "    \n",
    "# Apply the French language filter\n",
    "truly_en_dataset = dataset_tulu.filter(is_en_input, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "truly_en_dataset = truly_en_dataset.filter(is_en_response, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset_tulu['train'])\n",
    "filtered_count = len(truly_en_dataset['train'])\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original English-labeled examples: {original_count}\")\n",
    "print(f\"Actually English examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")\n",
    "\n",
    "# Count tokens\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HoangHa/Pensez-Llama3.1-8B\")\n",
    "\n",
    "def count_tokens(example, tokenizer):\n",
    "    \"\"\"Count tokens in messages that have a content field\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in example['messages']:\n",
    "        tokens = tokenizer(message['content'], return_tensors=\"pt\", truncation=False)\n",
    "        total_tokens += len(tokens['input_ids'][0])\n",
    "    example['token_count'] = total_tokens\n",
    "    return example\n",
    "    \n",
    "truly_en_dataset = truly_en_dataset.map(\n",
    "    lambda example: count_tokens(example, tokenizer), \n",
    "    num_proc=16\n",
    ")\n",
    "filterd_dataset = truly_en_dataset.filter(lambda x:  1024< x['token_count'] < 8192, num_proc=32)\n",
    "filterd_dataset\n",
    "\n",
    "# Filter multiturn\n",
    "import random\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Filter to create multiturn and singleturn datasets\n",
    "multiturn_samples = []\n",
    "singleturn_samples = []\n",
    "\n",
    "for sample in filterd_dataset['train']:\n",
    "    # Check number of messages to determine if multiturn or singleturn\n",
    "    if 2 < len(sample['messages']) < 9:\n",
    "        multiturn_samples.append(sample)\n",
    "    elif len(sample['messages']) == 2:\n",
    "        singleturn_samples.append(sample)\n",
    "\n",
    "# Create datasets from the filtered samples\n",
    "multiturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in multiturn_samples],\n",
    "    'messages': [sample['messages'] for sample in multiturn_samples],\n",
    "    'source': [sample['source'] for sample in multiturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in multiturn_samples],\n",
    "    'token_count': [sample['token_count'] for sample in multiturn_samples],\n",
    "})\n",
    "\n",
    "singleturn_dataset = Dataset.from_dict({\n",
    "    'id': [sample['id'] for sample in singleturn_samples],\n",
    "    'messages': [sample['messages'] for sample in singleturn_samples],\n",
    "    'source': [sample['source'] for sample in singleturn_samples],\n",
    "    'dataset': [sample['dataset'] for sample in singleturn_samples],\n",
    "    'token_count': [sample['token_count'] for sample in singleturn_samples]\n",
    "})\n",
    "\n",
    "# Create and print the dataset dictionaries\n",
    "multiturn_dataset_dict = DatasetDict({'train': multiturn_dataset})\n",
    "singleturn_dataset_dict = DatasetDict({'train': singleturn_dataset})\n",
    "\n",
    "print(\"Multiturn Dataset:\")\n",
    "print(multiturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(multiturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "print(\"\\nSingleturn Dataset:\")\n",
    "print(singleturn_dataset_dict)\n",
    "print(f\"Example conversation has {len(singleturn_dataset_dict['train'][0]['messages'])} messages\")\n",
    "\n",
    "# Verify distribution\n",
    "multiturn_count = len(multiturn_dataset)\n",
    "singleturn_count = len(singleturn_dataset)\n",
    "total_count = len(filterd_dataset['train'])\n",
    "\n",
    "print(f\"\\nDistribution Summary:\")\n",
    "print(f\"Total samples: {total_count}\")\n",
    "print(f\"Multiturn samples: {multiturn_count} ({multiturn_count/total_count:.2%})\")\n",
    "print(f\"Singleturn samples: {singleturn_count} ({singleturn_count/total_count:.2%})\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Extract number of messages per conversation\n",
    "message_lengths = [len(sample[\"messages\"]) for sample in multiturn_dataset]\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(message_lengths, bins=20, kde=True)\n",
    "plt.xlabel(\"Number of Messages per Conversation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Messages per Conversation\")\n",
    "plt.show()\n",
    "\n",
    "token_lengths = [sample[\"token_count\"] for sample in multiturn_dataset]\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(token_lengths, bins=20, kde=True)\n",
    "plt.xlabel(\"Number of Messages per Conversation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Messages per Conversation\")\n",
    "plt.show()\n",
    "\n",
    "multiturn_dataset_random= multiturn_dataset.shuffle(seed=42).select(range(91))\n",
    "# multiturn_dataset_random.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")\n",
    "multiturn_dataset_random[90]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Process each conversation in the dataset\n",
    "for idx, convo in enumerate(multiturn_dataset_random):\n",
    "    # Create a copy of the original conversation dictionary\n",
    "    updated_convo = dict(convo)\n",
    "    \n",
    "    # Add new formatted fields to the existing conversation\n",
    "    instruction_count, answer_count = 1, 1\n",
    "    for turn in convo[\"messages\"]:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            updated_convo[f\"instruction_{instruction_count}\"] = turn[\"content\"]\n",
    "            instruction_count += 1\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            updated_convo[f\"answer_{answer_count}\"] = turn[\"content\"]\n",
    "            answer_count += 1\n",
    "    \n",
    "    formatted_data.append(updated_convo)\n",
    "\n",
    "# Create updated dataset while preserving original structure\n",
    "updated_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Now updated_dataset contains all original fields plus the new instruction/answer fields\n",
    "# You can verify the changes\n",
    "print(f\"Original features: {multiturn_dataset_random.column_names}\")\n",
    "print(f\"Updated features: {updated_dataset.column_names}\")\n",
    "\n",
    "\n",
    "# # Display an example\n",
    "# print(formatted_dataset[0])\n",
    "\n",
    "# # Function to add `combine_x` columns\n",
    "# def add_combine_column(example):\n",
    "#     new_example = example.copy()\n",
    "#     i = 1\n",
    "#     while f\"instruction_{i}\" in example or f\"answer_{i}\" in example:\n",
    "#         instr = example.get(f\"instruction_{i}\")\n",
    "#         ans = example.get(f\"answer_{i}\")\n",
    "#         if instr is None or ans is None:\n",
    "#             new_example[f\"combine_{i}\"] = None\n",
    "#         else:\n",
    "#             new_example[f\"combine_{i}\"] = f\"Question:\\n{instr}\\nAnswer:\\n{ans}\\nSure, this is my thinking process.\\n\"\n",
    "#         i += 1\n",
    "#     return new_example\n",
    "\n",
    "# # Apply the transformation to create `combine_x` columns\n",
    "# formatted_dataset = formatted_dataset.map(add_combine_column)\n",
    "\n",
    "# formatted_dataset[10]\n",
    "\n",
    "# formatted_dataset.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8bc20-82d2-4518-9b93-4d495a15b1b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "singleturn_dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5907c-3145-43ca-a74f-e7cbc247fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleturn_dataset_random = singleturn_dataset_dict['train'].filter(lambda x:  512< x['token_count'] < 2048, num_proc=16)\n",
    "# singleturn_dataset_random.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645397e-2f87-4cb1-95c3-69ef2c7c3acf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "task_category_counts = Counter(singleturn_dataset_random['source'])\n",
    "print(task_category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28aa3c-abdd-45c6-b328-4a6d511197af",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleturn_dataset_random_filtered = singleturn_dataset_random.filter(\n",
    "    lambda x: 'math' not in x['source'].lower() and 'flan' not in x['source'].lower(),\n",
    "    num_proc=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c728c8-a815-46bf-b1cd-e626ae581730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "task_category_counts = Counter(singleturn_dataset_random_filtered['source'])\n",
    "print(task_category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfe3ed-8666-44b7-b313-593128a87575",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleturn_dataset_random_new = singleturn_dataset_random_filtered.shuffle(seed=42).select(range(140))\n",
    "singleturn_dataset_random_new.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26f865-4d6d-47e0-b070-b23adf552e4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "singleturn_dataset_random_new[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750ae43-ac9a-46df-99dd-7c51dca13927",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleturn_dataset_long = singleturn_dataset_dict['train'].filter(lambda x:  7000< x['token_count'] < 8192, num_proc=16)\n",
    "\n",
    "singleturn_dataset_long= singleturn_dataset_long.shuffle(seed=42).select(range(30))\n",
    "# singleturn_dataset_long.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40314dc2-7802-4a9e-afee-475f6c26dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleturn_dataset_long.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86098b-85e2-4530-9bbd-4b9b57c3d61f",
   "metadata": {},
   "source": [
    "# Reasoning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4766b-0744-4f52-8e07-ce1b03d741ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"GAIR/LIMO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0bfe2-c9a7-4a2d-a900-1ac8020313e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_answer(text):\n",
    "    boxed_pattern = r'\\\\boxed{([^}]*)}'\n",
    "    boxed_match = re.search(boxed_pattern, text)\n",
    "    if boxed_match:\n",
    "        return boxed_match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "extract_answer(dataset['train'][0]['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babe174-048c-4b72-90a1-69a196b79082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_answer(extracted, correct):\n",
    "    \"\"\"\n",
    "    Compare extracted answer with correct answer.\n",
    "    Both answers are normalized before comparison.\n",
    "    \"\"\" \n",
    "    return extracted == correct\n",
    "\n",
    "compare_answer(extract_answer(dataset['train'][0]['solution']),dataset['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe064d-bfef-4023-aa0c-ab66a939087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset and add correctness column for each generation\n",
    "    \"\"\"\n",
    "    def process_example(example):\n",
    "        extracted_answer = extract_answer(example['solution'])\n",
    "        is_correct = compare_answer(extracted_answer, example['answer'])\n",
    "        example['correctness'] = is_correct\n",
    "        return example\n",
    "        \n",
    "    processed_dataset = dataset.map(process_example, num_proc=32)\n",
    "    return processed_dataset\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_filtered = process_dataset(dataset).filter(lambda x: x['correctness'] == True)\n",
    "print(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8215c9b-3fea-40b0-8724-adac5caf9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(dataset, model_path=\"HoangHa/Pensez-v0.1-init\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def tokenize_and_count(example):\n",
    "        text = example[\"question\"] + example[\"solution\"]\n",
    "        return {\"token_count\": len(tokenizer(text).input_ids)}\n",
    "    \n",
    "    return dataset.map(tokenize_and_count)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_with_token_counts = count_tokens(dataset_filtered)\n",
    "print(dataset_with_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec7bc8-ca24-4802-bae0-eb7332dccbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts = dataset_with_token_counts.filter(lambda x:x['token_count']<15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a25ef-6816-49ea-a6ca-3b8262dcfb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be65de58-33b9-4741-af41-95c793c3bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts = dataset_with_token_counts['train'].shuffle(42).select(range(700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ea39d-7cda-4d89-9ccf-209105299e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-limo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec04e9c-4b1c-44bc-a06f-19da99f576c2",
   "metadata": {},
   "source": [
    "## FR-s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d0b0d-8406-41fe-8bf3-33c1a660dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare box\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract answer using multiple strategies in order of precedence:\n",
    "    1. \\boxed{...} format\n",
    "    2. Answer: format\n",
    "    3. Raw text as fallback\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "        \n",
    "    # Strategy 1: Extract from \\boxed{...}\n",
    "    boxed_pattern = r'\\\\boxed{([^}]*)}'\n",
    "    boxed_match = re.search(boxed_pattern, text)\n",
    "    if boxed_match:\n",
    "        return boxed_match.group(1).strip()\n",
    "        \n",
    "    # Strategy 2: Extract from \"Answer: ...\" format\n",
    "    answer_pattern = r'(?i)answer:\\s*(.+?)(?:\\n|$)'\n",
    "    answer_match = re.search(answer_pattern, text)\n",
    "    if answer_match:\n",
    "        return answer_match.group(1).strip()\n",
    "        \n",
    "    # Strategy 3: Use the raw text as fallback\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_answer_format(answer):\n",
    "    \"\"\"\n",
    "    Normalize different answer formats to a standard format 'A, B, C, D'\n",
    "    Handles various formats and separators including special characters.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return None\n",
    "        \n",
    "    # Convert to uppercase and strip whitespace\n",
    "    answer = answer.strip().upper()\n",
    "    \n",
    "    # If single character answer, return it\n",
    "    if len(answer) == 1 and answer.isalpha():\n",
    "        return answer\n",
    "    \n",
    "    # Step 1: Replace all non-alphanumeric characters with comma\n",
    "    standardized = re.sub(r'[^A-Z0-9]+', ',', answer)\n",
    "    \n",
    "    # Step 2: Handle case where options are just concatenated (e.g., 'ABCD')\n",
    "    if ',' not in standardized:\n",
    "        if all(c.isalpha() for c in standardized):\n",
    "            standardized = ','.join(list(standardized))\n",
    "    \n",
    "    # Step 3: Clean up the options\n",
    "    options = []\n",
    "    for opt in standardized.split(','):\n",
    "        opt = opt.strip()\n",
    "        if opt and (\n",
    "            (len(opt) == 1 and opt.isalpha()) or  # Single letter\n",
    "            opt.isdigit() or                      # Number\n",
    "            (len(opt) <= 2 and opt.isalnum())     # Alphanumeric up to 2 chars\n",
    "        ):\n",
    "            options.append(opt)\n",
    "    \n",
    "    # Step 4: Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    options = [x for x in options if not (x in seen or seen.add(x))]\n",
    "    \n",
    "    # Step 5: Join with standard separator\n",
    "    return ', '.join(options) if options else None\n",
    "\n",
    "def compare_answer(extracted, correct):\n",
    "    \"\"\"\n",
    "    Compare extracted answer with correct answer.\n",
    "    Both answers are normalized before comparison.\n",
    "    \"\"\"\n",
    "    if extracted is None or correct is None:\n",
    "        return False\n",
    "    \n",
    "    # First extract the answer using our new extraction logic\n",
    "    extracted_clean = extract_answer(extracted)\n",
    "    correct_clean = extract_answer(correct)\n",
    "    \n",
    "    # Then normalize both answers\n",
    "    normalized_extracted = normalize_answer_format(extracted_clean)\n",
    "    normalized_correct = normalize_answer_format(correct_clean)\n",
    "    \n",
    "    # Handle None cases after normalization\n",
    "    if normalized_extracted is None or normalized_correct is None:\n",
    "        return False\n",
    "    \n",
    "    return normalized_extracted == normalized_correct\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset and add correctness column for each generation\n",
    "    \"\"\"\n",
    "    def process_example(example):\n",
    "        extracted_answer = extract_answer(example['deepseek_attempt'])\n",
    "        is_correct = compare_answer(extracted_answer, example['solution'])\n",
    "        example['correctness'] = is_correct\n",
    "        return example\n",
    "        \n",
    "    processed_dataset = dataset.map(process_example, num_proc=32)\n",
    "    return processed_dataset\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset = load_dataset(\"simplescaling/s1K-1.1\", split='train')\n",
    "print(dataset)\n",
    "dataset_s1 = process_dataset(dataset).filter(lambda x: x['correctness'] == True)\n",
    "dataset_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf9cc3-6967-417f-bc5c-92507ff64336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset_s1[10]['gemini_thinking_trajectory'])\n",
    "print(\"-\"*50)\n",
    "print(dataset_s1[10]['deepseek_thinking_trajectory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c62231-98ba-48bc-a808-58f02318017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(dataset, model_path=\"HoangHa/Pensez-v0.1-init\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def tokenize_and_count(example):\n",
    "        text = example[\"question\"] + example[\"deepseek_thinking_trajectory\"] + example[\"deepseek_attempt\"]\n",
    "        return {\"token_count\": len(tokenizer(text).input_ids)}\n",
    "    \n",
    "    return dataset.map(tokenize_and_count)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_with_token_counts = count_tokens(dataset_s1)\n",
    "print(dataset_with_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b40ede-e11f-4aa3-8eb2-8ffa19520b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts = dataset_with_token_counts.filter(lambda x: x['token_count'] < 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8065861-430c-45b2-8a54-7f2757883969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_with_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c5947-52ec-4b80-bc70-63bfe5a86fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "science_samples = dataset_with_token_counts.filter(lambda x: x['cot_type'] == 'science')\n",
    "science_samples\n",
    "math_samples = dataset_with_token_counts.filter(lambda x: x['cot_type'] == 'math')\n",
    "new_dataset = concatenate_datasets([math_samples, science_samples])\n",
    "\n",
    "print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699abbe-a9dc-461b-8046-b51586b33d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_random = new_dataset.shuffle(42).select(range(142))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6507d-6a8b-496e-91e2-cf0943953356",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(new_dataset_random['token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279513bb-1a69-4457-94ea-45a2ab78fc1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/DeepHermes-3-Llama-3-8B-Preview\")\n",
    "\n",
    "# Calculate total tokens for each system\n",
    "gemini_totals = []\n",
    "deepseek_totals = []\n",
    "\n",
    "for example in dataset_s1:\n",
    "    # Gemini total (thinking + attempt)\n",
    "    gemini_total = len(tokenizer.encode(example[\"gemini_thinking_trajectory\"], add_special_tokens=True)) + \\\n",
    "                   len(tokenizer.encode(example[\"gemini_attempt\"], add_special_tokens=True))\n",
    "    gemini_totals.append(gemini_total)\n",
    "    \n",
    "    # DeepSeek total (thinking + attempt)\n",
    "    deepseek_total = len(tokenizer.encode(example[\"deepseek_thinking_trajectory\"], add_special_tokens=True)) + \\\n",
    "                     len(tokenizer.encode(example[\"deepseek_attempt\"], add_special_tokens=True))\n",
    "    deepseek_totals.append(deepseek_total)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot distributions with different colors and transparency\n",
    "plt.hist(gemini_totals, bins=50, alpha=0.5, label='Gemini Total', color='#8884d8')\n",
    "plt.hist(deepseek_totals, bins=50, alpha=0.5, label='DeepSeek Total', color='#82ca9d')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Total Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Total Tokens (Thinking + Attempt)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some basic statistics as text\n",
    "plt.text(0.02, 0.98, \n",
    "         f'Gemini Mean: {np.mean(gemini_totals):.0f}\\nDeepSeek Mean: {np.mean(deepseek_totals):.0f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics\n",
    "print(f\"\\nGemini Statistics:\")\n",
    "print(f\"Mean: {np.mean(gemini_totals):.2f}\")\n",
    "print(f\"Median: {np.median(gemini_totals):.2f}\")\n",
    "print(f\"Std Dev: {np.std(gemini_totals):.2f}\")\n",
    "\n",
    "print(f\"\\nDeepSeek Statistics:\")\n",
    "print(f\"Mean: {np.mean(deepseek_totals):.2f}\")\n",
    "print(f\"Median: {np.median(deepseek_totals):.2f}\")\n",
    "print(f\"Std Dev: {np.std(deepseek_totals):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b1da2-b54f-42e2-a49e-b9285766c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='answers.jsonl', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0996d6-8fb5-46cd-8322-aead5162a7fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"generations\", \"fr_deepseek_attempt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c825-6c49-4113-a6bb-5bda584c99bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HoangHa/Pensee\",\"en-reasoning-filtered\")\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145d43e-dfa1-4d8d-b383-87f11d5019d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_random.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-s1-reasoning-correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffebee-d33c-417a-8d3e-39fd5e044871",
   "metadata": {},
   "source": [
    "## Dolphin r1 - en-reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fadb8a-43e1-48cc-a1dc-54fa14d723d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cognitivecomputations/dolphin-r1\",\"reasoning-deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe58e5-106e-4450-ad49-757a0d9d9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_term_filtered = dataset.filter(\n",
    "    lambda x: '\\\\boxed{' in x['answer'].lower(),\n",
    "    num_proc=16\n",
    ")\n",
    "dataset_term_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d3898-88cb-42cb-ba8a-8826dc134476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_term_filtered['train'][100]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd258506-e25f-4628-9dae-78c0c3c5d0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(dataset, model_path=\"HoangHa/Pensez-v0.1-init\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def tokenize_and_count(example):\n",
    "        messages_text = \" \".join(msg[\"content\"] for msg in example[\"messages\"])\n",
    "        text = messages_text + \" \" + example[\"reasoning\"] + \" \" + example[\"answer\"]\n",
    "        return {\"token_count\": len(tokenizer(text).input_ids)}\n",
    "    \n",
    "    return dataset.map(tokenize_and_count, num_proc=64)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_with_token_counts = count_tokens(dataset_term_filtered)\n",
    "print(dataset_with_token_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673241ee-7031-4c4f-a749-5bde71e622c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dataset_with_token_counts['train']['token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebdec7-b55a-4b4f-9f96-818e355343a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_token_counts = dataset_with_token_counts['train'].filter(lambda x: x['token_count'] < 16384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1da769-7db9-4ca4-8119-22fda3388f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "# model.predict(\"Torrontés is a white grape variety native to Argentina, and its unique flavor profile is attributed to several factors. Here are some of the key reasons\")\n",
    "\n",
    "model.predict(\"Hello how are you? I'm Rex welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3be204-c9bc-4a8f-900f-15bcdcaac1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lang\n",
    "\n",
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_en_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['content'] for conv in conversations if conv['role'] == 'user']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:20000]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "\n",
    "def is_en_answer(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    gpt_responses = example['answer']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:20000]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "\n",
    "def is_en_reasoning(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    gpt_responses = example['reasoning']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:20000]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "    \n",
    "# Apply the French language filter\n",
    "truly_en_dataset = dataset_with_token_counts.filter(is_en_response, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "truly_en_dataset = truly_en_dataset.filter(is_en_answer, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "truly_en_dataset = truly_en_dataset.filter(is_en_reasoning, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset_with_token_counts)\n",
    "filtered_count = len(truly_en_dataset)\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original English-labeled examples: {original_count}\")\n",
    "print(f\"Actually English examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c7c14-cdf9-492c-b1f3-20f56801a988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_dataset_entries(dataset, start=0, end=None):\n",
    "    end = end if end is not None else len(dataset)  # Default to full dataset\n",
    "    for i in range(start, end):\n",
    "        entry = dataset[i]\n",
    "        print(f\"\\n=== Entry {i} from '' split ===\")\n",
    "        for key, value in entry.items():\n",
    "            print(f\"{key}: {value}\\n\")\n",
    "        print(\"=\" * 50)  # Separator for readability\n",
    "\n",
    "# Example usage: print all entries from the train split\n",
    "show_dataset_entries(truly_en_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e3b82-76dd-4d76-9180-ab5874f80b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "truly_en_dataset = truly_en_dataset.map(\n",
    "    lambda x: {'messages': x['messages'][0]['content']}, \n",
    "    num_proc=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c45408-d26e-4f6b-8c77-18dcdd90b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_dataset(dataset, split, exclude_indices):\n",
    "#     all_indices = list(range(len(dataset[split])))\n",
    "#     filtered_indices = [i for i in all_indices if i not in exclude_indices]\n",
    "#     return dataset[split].select(filtered_indices)\n",
    "\n",
    "# # Example usage:\n",
    "# exclude_indices = {25, 26, 30, 33, 41}\n",
    "# filtered_train_dataset = filter_dataset(truly_en_dataset, 'train', exclude_indices)\n",
    "# filtered_train_dataset.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single-reasoning-correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c6895-4c4b-420f-acdf-a087dc98e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select_dataset = truly_en_dataset.shuffle(seed=42).select(range(200))\n",
    "random_select_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052668fd-e802-416e-ba1b-929615c4782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select_dataset = random_select_dataset.rename_column(\"messages\", \"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21f20c-7f36-4622-8bfe-9ae5e56209b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select_dataset.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-dolphin-reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609ab7f-6175-4e8f-a1fb-177c8d9f8e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_select_dataset.push_to_hub(\"HoangHa/en-deepseek-r1-dolphin-random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa0c1b-f289-4d44-b99e-1c65d6ad9f41",
   "metadata": {},
   "source": [
    "# Currated Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe560c3-bce3-4ba2-bfb4-ccff845f7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"bethgelab/CuratedThoughts\", \"OpenR1-Math-220k-default\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2513ca4e-3163-42e8-9c45-fcff4691add3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35caea61-3c37-4038-a34a-52815c2490d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_en_response(example):\n",
    "    \"\"\"\n",
    "    Check if the response is actually in French using fasttext.\n",
    "    Returns True if the response is in French, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the GPT response\n",
    "    conversations = example['messages']\n",
    "    gpt_responses = [conv['value'] for conv in conversations if conv['from'] == 'user']\n",
    "    \n",
    "    if not gpt_responses:\n",
    "        return False\n",
    "    \n",
    "    # Use the first 500 characters for language detection\n",
    "    text_to_check = gpt_responses[0][:500]\n",
    "    \n",
    "    # Remove newlines and replace with spaces\n",
    "    text_to_check = text_to_check.replace('\\n', ' ')\n",
    "    \n",
    "    # Predict language\n",
    "    predictions = model.predict(text_to_check)\n",
    "    predicted_lang = predictions[0][0]\n",
    "    confidence = predictions[1][0]\n",
    "    \n",
    "    # Check if it's French with high confidence\n",
    "    is_eng = predicted_lang == '__label__eng_Latn' and confidence > 0.99\n",
    "    \n",
    "    return is_eng\n",
    "\n",
    "# Apply the French language filter\n",
    "truly_en_dataset = dataset.filter(is_en_response, num_proc=16)\n",
    "print(f\"Dataset after fasttext language verification: {truly_en_dataset}\")\n",
    "\n",
    "# See how many were removed\n",
    "original_count = len(dataset)\n",
    "filtered_count = len(truly_en_dataset)\n",
    "removed_count = original_count - filtered_count\n",
    "removed_percentage = (removed_count / original_count) * 100\n",
    "\n",
    "print(f\"Original English-labeled examples: {original_count}\")\n",
    "print(f\"Actually English examples: {filtered_count}\")\n",
    "print(f\"Removed {removed_count} examples ({removed_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755b013-70c4-4cd6-af0a-bc4d8d43dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_message(entry):\n",
    "#     try:\n",
    "#         for message in entry['messages']:\n",
    "#             if message['from'] == 'user':\n",
    "#                 message['value'] = re.sub(\n",
    "#                     r'^('\n",
    "#                     r'##?\\s*[\\w-]+\\s*|'              # Headers like \"## Task 4\"\n",
    "#                     r'(?:'                          # Non-capturing group for numbering patterns\n",
    "#                     r'\\.?\\s*\\[?\\d+\\]?\\s*|'          # Basic numbers like \"14. \"\n",
    "#                     r'(?:Example|Problem|Exercise)\\s*\\d+(?:\\.\\d+)*\\s*\\.?\\s*|' # \"Exercise 2. \"\n",
    "#                     r'[A-Za-z]?\\d+(?:\\.\\d+)*[A-Za-z]?\\s*\\.?\\s*|'    # Letter-number, including \"9.5. \"\n",
    "#                     r'Task\\s*\\d+/\\d+\\s*|'          # Task fractions\n",
    "#                     r'-\\s*\\d+\\s*|'                 # Dash numbers like \"- 260824\"\n",
    "#                     r'\\(\\d+\\s*points\\)\\s*|'        # Points like \"(5 points)\"\n",
    "#                     r'##\\s*Task\\s*\\d+\\s*-\\s*\\d+\\s*(?:\\n\\s*[a-z]\\)\\s*)?|' # \"## Task 4 - 260824\\n\\na) \"\n",
    "#                     r'\\d+(?:\\.\\d+)*\\s*\\.\\s*(?:\\n|\\(\\d+\\s*points\\)\\s*\\n)?|' # \"14. (5 points)\\n\"\n",
    "#                     r'\\.\\s*'                       # Standalone dot\n",
    "#                     r')'\n",
    "#                     r')',\n",
    "#                     '', \n",
    "#                     message['value'],\n",
    "#                     flags=re.IGNORECASE\n",
    "#                 ).strip()\n",
    "#         return entry\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error cleaning message: {e}\")\n",
    "#         return entry\n",
    "\n",
    "# # Apply to dataset\n",
    "# cleaned_dataset = truly_en_dataset.map(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcf93e-51b3-41fe-b806-fdbd46ed9ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count tokens\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HoangHa/Pensez-Llama3.1-8B\")\n",
    "\n",
    "def count_tokens(example, tokenizer):\n",
    "    \"\"\"Count tokens in messages that have a content field\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in example['messages']:\n",
    "        tokens = tokenizer(message['value'], return_tensors=\"pt\", truncation=False)\n",
    "        total_tokens += len(tokens['input_ids'][0])\n",
    "    example['token_count'] = total_tokens\n",
    "    return example\n",
    "    \n",
    "truly_en_dataset = truly_en_dataset.map(\n",
    "    lambda example: count_tokens(example, tokenizer), \n",
    "    num_proc=16\n",
    ")\n",
    "filterd_dataset = truly_en_dataset.filter(lambda x:  1024< x['token_count'] < 12000, num_proc=32)\n",
    "filterd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947d7d5-90be-474d-a0bd-ec7bbe6bf017",
   "metadata": {},
   "outputs": [],
   "source": [
    "truly_en_dataset_random = filterd_dataset.shuffle(seed=42).select(range(358))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc574b-f4ee-42ca-a3a0-cbaffe8be744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "truly_en_dataset_random[20]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ec08a-f0fc-4994-9f0c-f1f1e5a561c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "truly_en_dataset_random.push_to_hub(\"HoangHa/Pensez-v0.1\",\"fr-openmath-currated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d078fe-9c90-407f-b914-ad36a0d86d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "def extract_boxed_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the answer from \\\\boxed{...} pattern.\n",
    "    Returns None if no match is found.\n",
    "    \"\"\"\n",
    "    pattern = r'\\\\boxed{([^}]*)}'\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1).strip() if match else None\n",
    "    \n",
    "def create_conversation_format_ds(entry):\n",
    "    \"\"\"\n",
    "    Convert a dataset entry into the desired conversation format.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): Dictionary containing 'question' and 'solution' keys\n",
    "        \n",
    "    Returns:\n",
    "        list: List of conversation dictionaries\n",
    "    \"\"\"\n",
    "    # Create the conversation structure\n",
    "    conversations = [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": extract_boxed_answer(entry['fr_question'][0])\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": f\"<think>\\n{entry['deepseek_thinking_trajectory']}\\n</think>\\n\\n{entry['fr_deepseek_attempt'][0]}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load the dataset\n",
    "dataset_fr = load_dataset(\"HoangHa/Pensee\",\"en-reasoning-filtered\")\n",
    "\n",
    "# Add the conversations column to the dataset\n",
    "dataset_fr = dataset_fr.map(\n",
    "    lambda x: {'conversations': create_conversation_format_ds(x)},\n",
    "    remove_columns=None  # Keep all existing columns\n",
    ")\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Sample transformed entry:\")\n",
    "print(dataset_fr['train'][0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454e650-7c7b-4ae3-8cde-80b4b0c230bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def create_conversation_format(entry):\n",
    "    \"\"\"\n",
    "    Convert a dataset entry into the desired conversation format.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): Dictionary containing 'question' and 'solution' keys\n",
    "        \n",
    "    Returns:\n",
    "        list: List of conversation dictionaries\n",
    "    \"\"\"\n",
    "    solution = entry['solution']\n",
    "    \n",
    "    # Split into sentences (handling both '. ' and '.\\n' cases)\n",
    "    sentences = re.split(r'(?<=\\.)[\\s\\n]+', solution)\n",
    "    \n",
    "    # Remove empty sentences and trim each sentence\n",
    "    filtered_sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Get the last sentence and everything before it\n",
    "    last_sentence = filtered_sentences[-1]\n",
    "    reasoning = '. '.join(filtered_sentences[:-1])\n",
    "    \n",
    "    # Create the conversation structure\n",
    "    conversations = [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": entry['question']\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": f\"<think>\\n{reasoning}\\n</think>\\n\\n{last_sentence}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load the dataset\n",
    "dataset_limo = load_dataset(\"HoangHa/Pensee\", \"en-limo\")\n",
    "\n",
    "# Add the conversations column to the dataset\n",
    "dataset_limo = dataset_limo.map(\n",
    "    lambda x: {'conversations': create_conversation_format(x)},\n",
    "    remove_columns=None  # Keep all existing columns\n",
    ")\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"Sample transformed entry:\")\n",
    "print(dataset_limo['train'][0]['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c92c9d-b016-4d10-81d4-16791ee682da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset = concatenate_datasets([dataset_limo['train'], dataset_fr['train'], dataset_non['train']])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675d65a-72f8-432e-ba7c-9e17d02605f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem. If the question is for creative task (e.g., writing an essay, writing emails, translation, ...), you can have the confidence to answer directly without thinking.\"\"\"\n",
    "\n",
    "new_column = [SYSTEM_MESSAGE] * len(dataset)\n",
    "dataset = dataset.add_column(\"system\", new_column)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf1a21-1890-4c70-81c1-d29c7115774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_token_distribution(dataset):\n",
    "    \"\"\"Analyze token distribution of the training samples.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/DeepHermes-3-Llama-3-8B-Preview\")\n",
    "    token_counts = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        # Format the complete sample\n",
    "        sample_text = entry['system'] + \"\\n\"  # Start with system prompt\n",
    "        \n",
    "        # Add conversations\n",
    "        for conv in entry['conversations']:\n",
    "            sample_text += f\"{conv['from']}: {conv['value']}\\n\"\n",
    "        \n",
    "        # Count tokens\n",
    "        tokens = tokenizer.encode(sample_text)\n",
    "        token_counts.append(len(tokens))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"min_tokens\": np.min(token_counts),\n",
    "        \"max_tokens\": np.max(token_counts),\n",
    "        \"mean_tokens\": np.mean(token_counts),\n",
    "        \"median_tokens\": np.median(token_counts),\n",
    "        \"std_tokens\": np.std(token_counts),\n",
    "        \"percentiles\": {\n",
    "            \"25th\": np.percentile(token_counts, 25),\n",
    "            \"75th\": np.percentile(token_counts, 75),\n",
    "            \"90th\": np.percentile(token_counts, 90),\n",
    "            \"95th\": np.percentile(token_counts, 95),\n",
    "            \"99th\": np.percentile(token_counts, 99)\n",
    "        },\n",
    "        \"total_samples\": len(token_counts)\n",
    "    }\n",
    "    \n",
    "    return stats, token_counts\n",
    "\n",
    "def plot_token_distribution(token_counts):\n",
    "    \"\"\"Create a histogram of token counts.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(token_counts, bins=50, edgecolor='black')\n",
    "    plt.title('Distribution of Token Counts')\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt\n",
    "\n",
    "# Analyze train split\n",
    "print(\"Analyzing training set...\")\n",
    "train_stats, train_token_counts = analyze_token_distribution(dataset)\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nToken Distribution Statistics:\")\n",
    "print(f\"Total samples: {train_stats['total_samples']}\")\n",
    "print(f\"Minimum tokens: {train_stats['min_tokens']}\")\n",
    "print(f\"Maximum tokens: {train_stats['max_tokens']}\")\n",
    "print(f\"Mean tokens: {train_stats['mean_tokens']:.2f}\")\n",
    "print(f\"Median tokens: {train_stats['median_tokens']:.2f}\")\n",
    "print(f\"Standard deviation: {train_stats['std_tokens']:.2f}\")\n",
    "print(\"\\nPercentiles:\")\n",
    "for percentile, value in train_stats['percentiles'].items():\n",
    "    print(f\"{percentile}: {value:.2f}\")\n",
    "\n",
    "# Create visualization\n",
    "plt = plot_token_distribution(train_token_counts)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Analyze specific examples\n",
    "print(\"\\nAnalyzing some specific examples:\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/DeepHermes-3-Llama-3-8B-Preview\")\n",
    "\n",
    "# Look at shortest and longest examples\n",
    "min_idx = train_token_counts.index(min(train_token_counts))\n",
    "max_idx = train_token_counts.index(max(train_token_counts))\n",
    "\n",
    "for idx, label in [(min_idx, \"Shortest\"), (max_idx, \"Longest\")]:\n",
    "    sample = dataset[idx]\n",
    "    sample_text = sample['system'] + \"\\n\"\n",
    "    for conv in sample['conversations']:\n",
    "        sample_text += f\"{conv['from']}: {conv['value']}\\n\"\n",
    "    \n",
    "    tokens = tokenizer.encode(sample_text)\n",
    "    print(f\"\\n{label} sample (Index {idx}):\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"First 100 chars: {sample_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543c3d2-639d-4344-90c3-6839612732b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_messages_format(entry):\n",
    "    \"\"\"\n",
    "    Convert a dataset entry to the new messages format.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): Dictionary containing 'conversations' and 'system' keys\n",
    "        \n",
    "    Returns:\n",
    "        list: List containing a single dictionary with 'messages' key\n",
    "    \"\"\"\n",
    "    # Create messages array starting with system message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": entry['system']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Add conversations\n",
    "    for conv in entry['conversations']:\n",
    "        messages.append({\n",
    "            \"role\": \"user\" if conv['from'] == \"human\" else \"assistant\",\n",
    "            \"content\": conv['value']\n",
    "        })\n",
    "    \n",
    "    # Return in the required format\n",
    "    return messages\n",
    "\n",
    "# Add new column with converted format\n",
    "dataset = dataset.map(\n",
    "    lambda x: {'messages': convert_to_messages_format(x)},\n",
    "    remove_columns=None  # Keep existing columns\n",
    ")\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"\\nOriginal format:\")\n",
    "print(\"System:\", dataset[0]['system'][:100], \"...\")\n",
    "print(\"First conversation:\", dataset[0]['conversations'][0])\n",
    "\n",
    "print(\"\\nNew format:\")\n",
    "print(\"First message in new format:\")\n",
    "for msg in dataset[0]['messages_format'][0]['messages'][:2]:  # Show first 2 messages\n",
    "    print(f\"\\nRole: {msg['role']}\")\n",
    "    print(f\"Content: {msg['content'][:100]}...\")\n",
    "\n",
    "# Optional: Save the transformed dataset\n",
    "# dataset.save_to_disk(\"transformed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d378f5-d3aa-4a09-b6b0-4da43c7e3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['messages_format'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda3495-bc51-4fc1-90cb-86810830a376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[1597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec577f-f54c-4910-84db-6fece4f1da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.push_to_hub(\"HoangHa/Pensez\",\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b993b6-96d6-440f-ab4c-2aa4c0b6056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entries where any 'content' inside 'messages' is None\n",
    "invalid_ids = []\n",
    "for i, entry in enumerate(dataset):\n",
    "    if 'messages' in entry and isinstance(entry['messages'], list):\n",
    "        for msg in entry['messages']:\n",
    "            if not isinstance(msg, dict) or msg.get('content') is None:\n",
    "                invalid_ids.append(i)\n",
    "                break  # No need to check further messages in this entry\n",
    "\n",
    "print(\"IDs with None in 'content':\", invalid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f714c-f3e0-422a-a18b-f9abf728abd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[856]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261b603-4d76-4552-9f85-d646857a8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index to remove\n",
    "invalid_ids = [856]\n",
    "\n",
    "# Create a new dataset excluding the invalid index\n",
    "filtered_dataset = dataset.select([i for i in range(len(dataset)) if i not in invalid_ids])\n",
    "print(dataset)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a200348-b1eb-4923-90e2-c10b26eee171",
   "metadata": {},
   "source": [
    "# SGLANG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd2a15-aaf9-4e1c-9a58-0ef2001c1da1",
   "metadata": {},
   "source": [
    "FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/fr_full_sft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a755167-50a9-4497-9b52-86b259cbf9a7",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1, lm_eval --model vllm \\\n",
    "    --model_args pretrained=HoangHa/Pensez-Llama3.1-8B,dtype=auto,gpu_memory_utilization=0.9,data_parallel_size=1,max_model_len=16384,tensor_parallel_size=1 \\\n",
    "    --tasks french_bench \\\n",
    "    --limit 100 \\\n",
    "    --num_fewshot 5 \\\n",
    "    --batch_size auto:4 \\\n",
    "    --output_path data/french_bench/fr-8b/results_french_bench_5shot.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae40e0e-8204-41a0-9caa-5232684848a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch -m lm_eval --model hf \\\n",
    "    --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,max_length=16384,dtype=\"bfloat16\" \\\n",
    "    --tasks french_bench \\\n",
    "    --batch_size 16 \\\n",
    "    --num_fewshot 5 \\\n",
    "    --output_path data/french_bench/fr-8b/results_french_bench_5shot_qwen7b.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d64fc9-2106-4b0a-8f4c-ee5ee0fefd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m sglang.launch_server \\\n",
    "  --model-path HoangHa/Pensez-Llama3.1-8B \\\n",
    "  --port 30000 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --dp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa30531-969a-4066-ac7a-0b40019184c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m sglang.launch_server \\\n",
    "  --model-path meta-llama/Llama-3.3-70B-Instruct \\\n",
    "  --port 30000 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --tp 4 \\\n",
    "  --dp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84eda5-fe3c-457d-81e4-8c895698cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HoangHa/math-lv5-fr\n",
    "# HoangHa/gpqa-fr\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/math-lv5-fr\" \\\n",
    "  --output-file \"math_lv5_fr.jsonl\" \\\n",
    "  --prompt-column \"problem\" \\\n",
    "  --uuid-column \"problem\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686d8e9-406a-4b60-8b0b-f417d447106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"HoangHa/Pensez-v0.1\", \"fr-qwen-single\"\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-mt\" \\\n",
    "  --output-file \"en-mt.jsonl\" \\\n",
    "  --prompt-column \"combine_1\" \\\n",
    "  --uuid-column \"combine_1\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9c294-cc18-4864-aa83-d772afdb61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/fr_full_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d5e03-5687-45a0-8c47-685bce2c1897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# Load the medqa dataset (both train and dev splits)\n",
    "train_dataset = load_dataset(\"le-leadboard/MATH_LVL5_fr\", \"default\", split='train')\n",
    "train_dataset.push_to_hub(\"HoangHa/math-lv5-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500c190-e7ed-42dd-a115-439e86add996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Data for the three models\n",
    "models = [\"Pensez v0.1 e5\", \"R1 distil\", \"Qwen 7B instruct\"]\n",
    "metrics = [\"Math-hard (fr)\", \"MMLU (fr)\", \"Boolqa (fr)\", \"Trivia (en)\", \"Hellaswag (en)\"]\n",
    "\n",
    "# Values from the provided data\n",
    "data = {\n",
    "    \"Pensez v0.1 e5\": [0.3458, 0.5766, 0.9157, 0.4421, 0.5050],\n",
    "    \"R1 distil\": [0.3403, 0.4961, 0.7079, 0.2711, 0.3540],\n",
    "    \"Qwen 7B instruct\": [0.2253, 0.6612, 0.9382, 0.5316, 0.5258]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for easy reference\n",
    "df = pd.DataFrame(data, index=metrics)\n",
    "print(df)\n",
    "\n",
    "# Number of metrics (vertices of pentagon)\n",
    "N = len(metrics)\n",
    "\n",
    "# Calculate the angle for each metric (in radians)\n",
    "angles = [n / N * 2 * np.pi for n in range(N)]\n",
    "# Make the plot close by appending the first angle again\n",
    "angles += angles[:1]\n",
    "\n",
    "# Set up the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Define colors for each model\n",
    "colors = ['#4285F4', '#EA4335', '#34A853']  # Blue, Red, Green\n",
    "alphas = [0.3, 0.3, 0.3]  # Transparency for fill\n",
    "\n",
    "# Plot each model\n",
    "for i, model in enumerate(models):\n",
    "    # Get values for the current model\n",
    "    values = df[model].tolist()\n",
    "    # Close the polygon by appending the first value\n",
    "    values += values[:1]\n",
    "    \n",
    "    # Plot the values\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "    # Fill the area\n",
    "    ax.fill(angles, values, alpha=alphas[i], color=colors[i])\n",
    "\n",
    "# Fix axis to go in the right order and start at 12 o'clock\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Draw axis lines for each angle and label\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.tick_params(axis='x', labelsize=9)\n",
    "\n",
    "# Set y axis limits and labels\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n",
    "\n",
    "# Remove grid lines and spines from the center\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.spines['polar'].set_visible(False)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "# Add title\n",
    "plt.title('Model Performance Comparison', size=15, y=1.1)\n",
    "\n",
    "# Create a more clean and visible visualization\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Add concentric circles for reference points\n",
    "for r in [0.2, 0.4, 0.6, 0.8]:\n",
    "    circle = plt.Circle((0, 0), r, transform=ax.transData._b, fill=False, \n",
    "                       edgecolor='gray', alpha=0.3, linestyle='--')\n",
    "    ax.add_artist(circle)\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_pentagon_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Create a separate figure for the table\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "# Format data for table\n",
    "table_data = []\n",
    "for metric in metrics:\n",
    "    row = []\n",
    "    for model in models:\n",
    "        row.append(f\"{df.loc[metric, model]*100:.2f}%\")\n",
    "    table_data.append(row)\n",
    "\n",
    "# Create the table\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    rowLabels=metrics,\n",
    "    colLabels=models,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    bbox=[0, 0, 1, 1]\n",
    ")\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Color headers\n",
    "for i in range(len(models) + 1):\n",
    "    table[(0, i)].set_facecolor('#f2f2f2')\n",
    "    \n",
    "for i in range(len(metrics)):\n",
    "    table[(i + 1, 0)].set_facecolor('#f2f2f2')\n",
    "\n",
    "plt.savefig('model_performance_table.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Summary of key findings\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"-----------------\")\n",
    "print(\"1. Qwen 7B instruct performs best on most metrics, particularly on MMLU (fr), Boolqa (fr), and Trivia (en)\")\n",
    "print(\"2. Pensez v0.1 e5 has strong performance on Boolqa (fr) and is competitive on Hellaswag (en)\")\n",
    "print(\"3. R1 distil generally underperforms compared to the other models except on Math-hard (fr) where it's close to Pensez\")\n",
    "print(\"4. All models struggle the most with Math-hard (fr)\")\n",
    "print(\"5. Boolqa (fr) is the task where all models perform best\")\n",
    "\n",
    "# Calculate the average performance for each model\n",
    "print(\"\\nAverage Performance:\")\n",
    "print(\"-------------------\")\n",
    "for model in models:\n",
    "    print(f\"{model}: {df[model].mean()*100:.2f}%\")\n",
    "\n",
    "# Find the best model for each metric\n",
    "best_models = df.idxmax(axis=1)\n",
    "print(\"\\nBest Model per Metric:\")\n",
    "print(\"---------------------\")\n",
    "for metric in metrics:\n",
    "    best_model = best_models[metric]\n",
    "    print(f\"{metric}: {best_model} ({df.loc[metric, best_model]*100:.2f}%)\")\n",
    "\n",
    "plt.close('all')  # Close all figures to avoid display in notebook environments\n",
    "\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"- model_pentagon_comparison.png - Pentagon chart with all three models\")\n",
    "print(\"- model_performance_table.png - Table with performance values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5dc83-2ee7-4da6-9887-bced140e194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"parquet\", data_files=\"/home/jovyan/visual-thinker-workspace/open-r1/data/aime/pensez-e5/details/HoangHa/Pensez-v0.1-e5/2025-03-02T10-45-36.571132/aime25.parquet\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8991bc1-5534-4fa9-9d12-e4a3610a8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"parquet\", data_files=\"/home/jovyan/visual-thinker-workspace/open-r1/data/aime/pensez-e5/details/HoangHa/Pensez-v0.1-e5/2025-03-02T10-45-36.571132/aime24.parquet\")\n",
    "\n",
    "# Function to count occurrences of \"wait\" in a text (case-insensitive)\n",
    "def count_wait(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    # Count occurrences of \"wait\" as a whole word (case-insensitive)\n",
    "    return len(re.findall(r'\\bwait\\b', text.lower()))\n",
    "\n",
    "# Process each example in the training set\n",
    "correct_data = {'prediction_id': [], 'wait_count': []}\n",
    "incorrect_data = {'prediction_id': [], 'wait_count': []}\n",
    "\n",
    "for i, example in enumerate(data['train']):\n",
    "    prediction = example['predictions']\n",
    "    extractive_match = example['metrics']['extractive_match']\n",
    "    \n",
    "    wait_count = count_wait(prediction[0])\n",
    "    \n",
    "    if extractive_match == 1:  # Correct prediction\n",
    "        correct_data['prediction_id'].append(i)\n",
    "        correct_data['wait_count'].append(wait_count)\n",
    "    else:  # Incorrect prediction\n",
    "        incorrect_data['prediction_id'].append(i)\n",
    "        incorrect_data['wait_count'].append(wait_count)\n",
    "\n",
    "# Convert to DataFrames for easier analysis\n",
    "correct_df = pd.DataFrame(correct_data)\n",
    "incorrect_df = pd.DataFrame(incorrect_data)\n",
    "\n",
    "# Calculate statistics\n",
    "total_correct = len(correct_df)\n",
    "total_incorrect = len(incorrect_df)\n",
    "total_wait_correct = correct_df['wait_count'].sum()\n",
    "total_wait_incorrect = incorrect_df['wait_count'].sum()\n",
    "avg_wait_correct = total_wait_correct / total_correct if total_correct > 0 else 0\n",
    "avg_wait_incorrect = total_wait_incorrect / total_incorrect if total_incorrect > 0 else 0\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Category': ['Correct (extractive_match=1)', 'Incorrect (extractive_match=0)'],\n",
    "    'Count': [total_correct, total_incorrect],\n",
    "    'Total Wait Words': [total_wait_correct, total_wait_incorrect],\n",
    "    'Average Wait Per Prediction': [avg_wait_correct, avg_wait_incorrect]\n",
    "})\n",
    "\n",
    "# Create distribution DataFrame for plotting\n",
    "correct_df['category'] = 'Correct'\n",
    "incorrect_df['category'] = 'Incorrect'\n",
    "plot_df = pd.concat([correct_df, incorrect_df])\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Bar chart comparing averages\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(['Correct', 'Incorrect'], [avg_wait_correct, avg_wait_incorrect], color=['green', 'red'])\n",
    "plt.title('Average \"Wait\" Occurrences per Prediction')\n",
    "plt.ylabel('Average Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2. Box plot showing distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='category', y='wait_count', data=plot_df, palette=['green', 'red'])\n",
    "plt.title('Distribution of \"Wait\" Occurrences')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 3. Histogram of wait counts by category\n",
    "plt.subplot(2, 2, 3)\n",
    "bins = np.arange(0, max(plot_df['wait_count']) + 2) - 0.5\n",
    "sns.histplot(data=plot_df, x='wait_count', hue='category', multiple='dodge', \n",
    "             bins=bins, palette=['green', 'red'], discrete=True)\n",
    "plt.title('Histogram of \"Wait\" Occurrences')\n",
    "plt.xlabel('Number of \"Wait\" in Prediction')\n",
    "plt.ylabel('Count of Predictions')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 4. Scatter plot of prediction ID vs wait count\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(correct_df['prediction_id'], correct_df['wait_count'], \n",
    "            label='Correct', color='green', alpha=0.7)\n",
    "plt.scatter(incorrect_df['prediction_id'], incorrect_df['wait_count'], \n",
    "            label='Incorrect', color='red', alpha=0.7)\n",
    "plt.title('Prediction ID vs \"Wait\" Count')\n",
    "plt.xlabel('Prediction ID')\n",
    "plt.ylabel('Wait Count')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wait_analysis_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Print numerical summary\n",
    "print(\"Summary Statistics:\")\n",
    "print(summary)\n",
    "\n",
    "# Calculate correlation between wait count and correctness\n",
    "correct_vals = correct_df['wait_count'].tolist()\n",
    "incorrect_vals = incorrect_df['wait_count'].tolist()\n",
    "\n",
    "print(\"\\nStatistical Analysis:\")\n",
    "print(f\"Correct predictions: mean={np.mean(correct_vals):.2f}, median={np.median(correct_vals):.2f}, max={np.max(correct_vals)}\")\n",
    "print(f\"Incorrect predictions: mean={np.mean(incorrect_vals):.2f}, median={np.median(incorrect_vals):.2f}, max={np.max(incorrect_vals)}\")\n",
    "\n",
    "# t-test to check if the difference is statistically significant\n",
    "from scipy import stats\n",
    "t_stat, p_val = stats.ttest_ind(correct_vals, incorrect_vals, equal_var=False)\n",
    "print(f\"\\nStatistical significance (t-test): t={t_stat:.2f}, p={p_val:.4f}\")\n",
    "print(f\"Interpretation: {'Statistically significant difference' if p_val < 0.05 else 'No statistically significant difference'}\")\n",
    "\n",
    "# Count how many predictions have at least one \"wait\"\n",
    "correct_with_wait = sum(1 for count in correct_vals if count > 0)\n",
    "incorrect_with_wait = sum(1 for count in incorrect_vals if count > 0)\n",
    "print(f\"\\nPredictions with at least one 'wait':\")\n",
    "print(f\"Correct: {correct_with_wait}/{total_correct} ({correct_with_wait/total_correct*100:.1f}%)\")\n",
    "print(f\"Incorrect: {incorrect_with_wait}/{total_incorrect} ({incorrect_with_wait/total_incorrect*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
