{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a692dcf2-27e0-477d-bfe9-d642dc7fcc6b",
   "metadata": {},
   "source": [
    "# French translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffd3c8-98d3-4d60-b1b5-91a14979f110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-dolphin-reasoning (regen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d95064-5d59-455d-b241-93a7eee53691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_dolphin_reasoning = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-dolphin-reasoning\", split='train')\n",
    "dataset_dolphin_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec837b-f790-4eff-b1e1-c027935ba374",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"fr-dolphin-reasoning\" \\\n",
    "  --output-file \"fr-dolphin-reasoning-question.jsonl\" \\\n",
    "  --prompt-column \"prompt\" \\\n",
    "  --uuid-column \"prompt\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89feb1b9-7db3-499d-8451-72035a046a49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"fr-dolphin-reasoning\" \\\n",
    "  --output-file \"fr-dolphin-reasoning-answer.jsonl\" \\\n",
    "  --prompt-column \"answer\" \\\n",
    "  --uuid-column \"answer\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62312e8b-dc6f-46ad-beb5-9b6cf29e4afe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset(file_path, original_col, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.select_columns([original_col, new_col])\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_dolphin_reasoning_question = process_generations_dataset('fr-dolphin-reasoning-question.jsonl', 'prompt', 'fr_prompt')\n",
    "dataset_dolphin_reasoning_answer = process_generations_dataset('fr-dolphin-reasoning-answer.jsonl', 'answer', 'fr_answer')\n",
    "\n",
    "# Print results\n",
    "print(dataset_dolphin_reasoning_question)\n",
    "print(dataset_dolphin_reasoning_question[0])\n",
    "print(dataset_dolphin_reasoning_answer)\n",
    "print(dataset_dolphin_reasoning_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c6078-cda6-4ad8-ab68-f769aa85d7fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to dictionaries for fast lookup\n",
    "question_dict = {x['prompt']: x['fr_prompt'] for x in dataset_dolphin_reasoning_question}\n",
    "answer_dict = {x['answer']: x['fr_answer'] for x in dataset_dolphin_reasoning_answer}\n",
    "\n",
    "# Merge based on exact match\n",
    "def merge_rows(row):\n",
    "    row['fr_prompt'] = question_dict.get(row['prompt'], '')  # Add fr_prompt if prompt matches\n",
    "    row['fr_answer'] = answer_dict.get(row['answer'], '')    # Add fr_answer if answer matches\n",
    "    return row\n",
    "\n",
    "# Apply merging\n",
    "dataset_dolphin_reasoning = dataset_dolphin_reasoning.map(merge_rows)\n",
    "\n",
    "print(dataset_dolphin_reasoning)\n",
    "dataset_dolphin_reasoning[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b6628-65af-4d75-98d2-2677e231cb14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_dolphin_reasoning.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-dolphin-reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fb982-7f02-4458-ae51-858637aa81ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with customizable column names\n",
    "dataset_fr_dolphin_openai_messages = convert_to_openai_messages(dataset_dolphin_reasoning,\n",
    "                                                     prompt_col=\"fr_prompt\",\n",
    "                                                     reasoning_col=\"reasoning\",\n",
    "                                                     answer_col=\"fr_answer\",\n",
    "                                                    source=\"fr-dolphin-reasoning\")\n",
    "\n",
    "# Print results\n",
    "print(dataset_fr_dolphin_openai_messages)\n",
    "print(dataset_fr_dolphin_openai_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880da603-fac9-488f-80fa-b50bd09a7dcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-s1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6acce58a-8921-4ec7-8c48-1bf623b3890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_s1_reasoning = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-s1-reasoning-correct\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27df52c-710f-40ed-86e6-122abf9fb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1_reasoning.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88890e05-8439-4373-8b54-734d2e4a544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "target_text = \"\"\"Particules non relativistes de masse m1, provenant d'une source à température T1, passent à travers un filtre qui sélectionne uniquement celles qui ont une vitesse proche de la moyenne (de toutes les particules de la source), puis elles illuminent une fente de largeur 2,5 micromètres. Sur un écran d'observation, de l'autre côté de la fente, une série de régions claires et sombres est observée, la première région claire (la plus proche du centre de la fente) étant située à un angle de 15 degrés. Plus tard, la même procédure est répétée, mais cette fois, la source est remplacée par une source contenant des particules de masse m2 = 2 * m1 et de température T2 ; la deuxième région claire apparaît maintenant à 20 degrés. Toutes les distances et les angles sont mesurés à partir de, ou par rapport à, une ligne reliant le centre de la fente et le point de l'écran situé juste en face. Quelle est la valeur du rapport T1/T2 ? \\nChoix de réponses :\\n(A) 2,2\\n(B) 1,3\\n(C) 3,5\\n(D) 1,9\"\"\"\n",
    "\n",
    "for i, entry in enumerate(dataset_s1_reasoning):\n",
    "    if entry[\"fr_prompt\"] == target_text:\n",
    "        print(\"Index:\", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65851d3-ae46-40a6-8e76-677bc681b1aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"fr-s1-reasoning-correct\" \\\n",
    "  --output-file \"fr-s1-reasoning-correct-question.jsonl\" \\\n",
    "  --prompt-column \"question\" \\\n",
    "  --uuid-column \"question\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bea039-95d4-4ca7-890a-940744727874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"fr-s1-reasoning-correct\" \\\n",
    "  --output-file \"fr-s1-reasoning-correct-answer.jsonl\" \\\n",
    "  --prompt-column \"deepseek_attempt\" \\\n",
    "  --uuid-column \"deepseek_attempt\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2857e0b-c816-4ea7-88d6-ec9d1b0ed861",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset(file_path, original_col, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.select_columns([original_col, new_col])\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_s1_reasoning_question = process_generations_dataset('fr-s1-reasoning-correct-question.jsonl', 'question', 'fr_prompt')\n",
    "dataset_s1_reasoning_answer = process_generations_dataset('fr-s1-reasoning-correct-answer.jsonl', 'deepseek_attempt', 'fr_answer')\n",
    "\n",
    "# Print results\n",
    "print(dataset_s1_reasoning_question)\n",
    "print(dataset_s1_reasoning_question[0])\n",
    "print(dataset_s1_reasoning_answer)\n",
    "print(dataset_s1_reasoning_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea9419-15ee-4818-8305-e76838bc065a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to dictionaries for fast lookup\n",
    "question_dict = {x['question']: x['fr_prompt'] for x in dataset_s1_reasoning_question}\n",
    "answer_dict = {x['deepseek_attempt']: x['fr_answer'] for x in dataset_s1_reasoning_answer}\n",
    "\n",
    "# Merge based on exact match\n",
    "def merge_rows(row):\n",
    "    row['fr_prompt'] = question_dict.get(row['question'], '')  # Add fr_prompt if prompt matches\n",
    "    row['fr_answer'] = answer_dict.get(row['deepseek_attempt'], '')    # Add fr_answer if answer matches\n",
    "    return row\n",
    "\n",
    "# Apply merging\n",
    "dataset_s1_reasoning = dataset_s1_reasoning.map(merge_rows)\n",
    "\n",
    "print(dataset_s1_reasoning)\n",
    "dataset_s1_reasoning[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea90ab-820f-4554-abee-8ed501bcc1c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset_s1_reasoning = dataset_s1_reasoning.remove_columns([\"fr_deepseek_attempt\", \"fr_question\"])\n",
    "dataset_s1_reasoning.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-s1-reasoning-correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a00a40-86df-4c7d-a0f4-7070bfad1646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_fr_s1_openai_messages = convert_to_openai_messages(\n",
    "    dataset_s1_reasoning,\n",
    "    prompt_col=\"fr_prompt\",\n",
    "    reasoning_col=\"deepseek_thinking_trajectory\",\n",
    "    answer_col=\"fr_answer\",\n",
    "    source=\"fr-s1-reasoning-correct\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_fr_s1_openai_messages)\n",
    "print(dataset_fr_s1_openai_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47be27-7a4e-4a0b-a733-b031b4576e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e37e02e-75c8-40fb-99d8-2f75e3218af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-OpenMath-Currated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878d269-66fa-4d1f-bcd7-17a9a11aa191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_openmath_reasoning = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-openmath-currated\", split='train')\n",
    "dataset_openmath_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b8990-a639-4b57-a7b1-f85389aaad64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def process_messages(example):\n",
    "    \"\"\"\n",
    "    Process the 'messages' field to extract question, output, and reasoning into new columns.\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the dataset containing 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with new columns 'question', 'output', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    question = \"\"\n",
    "    output = \"\"\n",
    "    reasoning = \"\"\n",
    "    \n",
    "    # Check if 'messages' exists and is a list\n",
    "    if 'messages' not in example or not isinstance(example['messages'], list):\n",
    "        return {'question': question, 'output': output, 'reasoning': reasoning}\n",
    "    \n",
    "    # Process each message in the list\n",
    "    for message in example['messages']:\n",
    "        if message.get('from') == 'user':\n",
    "            # Extract the question from user message\n",
    "            question = message.get('value', '').strip()\n",
    "        \n",
    "        elif message.get('from') == 'assistant':\n",
    "            # Extract the full response from assistant\n",
    "            response = message.get('value', '')\n",
    "            \n",
    "            # Split into reasoning (between <think> and </think>) and output (after </think>)\n",
    "            reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "            \n",
    "            # Output is everything after </think>, if it exists\n",
    "            output_start = response.find('</think>')\n",
    "            if output_start != -1:\n",
    "                output = response[output_start + 8:].strip()  # 8 is length of </think>\n",
    "            else:\n",
    "                output = response.strip()  # If no </think>, take the whole response\n",
    "            \n",
    "            # Clean up any residual tags (in case of malformed input)\n",
    "            reasoning = re.sub(r'<think>|</think>', '', reasoning).strip()\n",
    "            output = re.sub(r'<think>|</think>', '', output).strip()\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'output': output,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "# Load the dataset\n",
    "dataset_openmath_reasoning = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-openmath-currated\", split='train')\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "dataset_openmath_reasoning = dataset_openmath_reasoning.map(process_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33472f1b-a0f7-48a9-a608-e91f290c9446",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_openmath_reasoning.push_to_hub(\"HoangHa/Pensez-v0.1\",\"fr-openmath-currated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a47e99-340e-4311-9e84-ca703ccc4a2a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset(file_path, original_col, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.select_columns([original_col, new_col])\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_openmath_reasoning_question = process_generations_dataset('fr-openmath-question.jsonl', 'question', 'fr_prompt')\n",
    "dataset_openmath_reasoning_answer = process_generations_dataset('fr-openmath-answer.jsonl', 'output', 'fr_answer')\n",
    "\n",
    "# Print results\n",
    "print(dataset_openmath_reasoning_question)\n",
    "print(dataset_openmath_reasoning_question[0])\n",
    "print(dataset_openmath_reasoning_answer)\n",
    "print(dataset_openmath_reasoning_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b0c31-64d6-4018-8e05-d44514c604e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to dictionaries for fast lookup\n",
    "question_dict = {x['question']: x['fr_prompt'] for x in dataset_openmath_reasoning_question}\n",
    "answer_dict = {x['output']: x['fr_answer'] for x in dataset_openmath_reasoning_answer}\n",
    "\n",
    "# Merge based on exact match\n",
    "def merge_rows(row):\n",
    "    row['fr_prompt'] = question_dict.get(row['question'], '')  # Add fr_prompt if prompt matches\n",
    "    row['fr_answer'] = answer_dict.get(row['output'], '')    # Add fr_answer if answer matches\n",
    "    return row\n",
    "\n",
    "# Apply merging\n",
    "dataset_openmath_reasoning = dataset_openmath_reasoning.map(merge_rows)\n",
    "\n",
    "print(dataset_openmath_reasoning)\n",
    "dataset_openmath_reasoning[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c604b5f-7f90-4a29-8e2f-77b93dfbf63b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_openmath_reasoning.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-openmath-currated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3912e-e7cf-4602-8772-209d92574052",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_fr_openmath_openai_messages = convert_to_openai_messages(\n",
    "    dataset_openmath_reasoning,\n",
    "    prompt_col=\"fr_prompt\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"fr_answer\",\n",
    "    source=\"fr-openmath-currated\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_fr_openmath_openai_messages)\n",
    "print(dataset_fr_openmath_openai_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea880194-7692-479a-a7e9-e7d66144cab8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafda8c-4197-4d03-816f-1c240d388ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_single_fr = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-single\", split='train')\n",
    "dataset_single_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd61a1-6ed9-43d8-abe3-357926f0b9b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def process_messages(example):\n",
    "    \"\"\"\n",
    "    Process the 'messages' field to extract question, output, and reasoning into new columns.\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the dataset containing 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with new columns 'question', 'output', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    reasoning = \"\"\n",
    "    combine = \"\"\n",
    "    \n",
    "    # Check if 'messages' exists and is a list\n",
    "    if 'messages' not in example or not isinstance(example['messages'], list):\n",
    "        return {'question': question, 'output': output, 'reasoning': reasoning}\n",
    "    \n",
    "    # Process each message in the list\n",
    "    for message in example['messages']:\n",
    "        if message.get('role') == 'user':\n",
    "            # Extract the question from user message\n",
    "            question = message.get('content', '').strip()\n",
    "        \n",
    "        elif message.get('role') == 'assistant':\n",
    "            # Extract the full response from assistant\n",
    "            response = message.get('content', '')\n",
    "            \n",
    "            # Split into reasoning (between <think> and </think>) and output (after </think>)\n",
    "            reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "            \n",
    "            # Output is everything after </think>, if it exists\n",
    "            output_start = response.find('</think>')\n",
    "            if output_start != -1:\n",
    "                output = response[output_start + 8:].strip()  # 8 is length of </think>\n",
    "            else:\n",
    "                output = response.strip()  # If no </think>, take the whole response\n",
    "            \n",
    "            # Clean up any residual tags (in case of malformed input)\n",
    "            reasoning = re.sub(r'<think>|</think>', '', reasoning).strip()\n",
    "            output = re.sub(r'<think>|</think>', '', output).strip()\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': output,\n",
    "        'reasoning': reasoning,\n",
    "        'combine': \"Question:\\n\" + question + \"\\Answer:\\n\" + output + \"\\Sure, this is my thinking process.\\n\"\n",
    "    }\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "dataset_single_fr = dataset_single_fr.map(process_messages)\n",
    "dataset_single_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d1973-9c79-41a8-af7f-15ea4c190dec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_single_fr[0]['combine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f54be-5bc3-4876-a68e-b624a82c2e78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_single_fr.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1678b2f-a55d-45f0-b0f9-00381acf96b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_single_fr_json = process_generations_dataset_combine('fr-single.jsonl', 'reasoning')\n",
    "\n",
    "# Print results\n",
    "print(dataset_single_fr_json)\n",
    "dataset_single_fr_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd62e6-3969-4d7e-9ec1-1752a73ccd2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_single_fr_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7f77a-500f-45c9-a763-5134bc283072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_single_fr_openai_messages = convert_to_openai_messages(\n",
    "    dataset_single_fr,\n",
    "    prompt_col=\"question\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"answer\",\n",
    "    source=\"fr-single\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_single_fr_openai_messages)\n",
    "dataset_single_fr_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e2c5c-8048-44ce-b707-6410f8e8065c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-llama-single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cb0cb-6492-488e-ba19-44e65d3583af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_llama_single_fr = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-llama-single\", split='train')\n",
    "dataset_llama_single_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb5fc7-7a26-412c-b696-aa30dcfcd6dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    example['combine'] = f\"Question:\\n{example['instruction']}\\nAnswer:\\n{example['response']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_llama_single_fr = dataset_llama_single_fr.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_llama_single_fr[0]['combine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfca1e-ecea-4cad-bede-ded736ac80b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_llama_single_fr_json = process_generations_dataset_combine('fr-llama-single.jsonl', 'reasoning')\n",
    "\n",
    "# Print results\n",
    "print(dataset_llama_single_fr_json)\n",
    "dataset_llama_single_fr_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61504a8c-7e1b-4d50-9196-a808d8afe9e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_llama_single_fr_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-llama-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16782ada-7edd-432a-9872-5cb06b06db28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_single_llama_fr_openai_messages = convert_to_openai_messages(\n",
    "    dataset_llama_single_fr,\n",
    "    prompt_col=\"instruction\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"response\",\n",
    "    source=\"fr-llama-single\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_single_llama_fr_openai_messages)\n",
    "dataset_single_llama_fr_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f4232-8055-4e69-83d9-d0a819829a96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943251eb-9118-4061-9c14-2723eb0aaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_long_fr = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-long\", split='train')\n",
    "dataset_long_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b4473-a7d6-4b58-aa42-e7bdd5926095",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def process_messages(example):\n",
    "    \"\"\"\n",
    "    Process the 'messages' field to extract question, output, and reasoning into new columns.\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the dataset containing 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with new columns 'question', 'output', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    reasoning = \"\"\n",
    "    combine = \"\"\n",
    "    \n",
    "    # Check if 'messages' exists and is a list\n",
    "    if 'messages' not in example or not isinstance(example['messages'], list):\n",
    "        return {'question': question, 'output': output, 'reasoning': reasoning}\n",
    "    \n",
    "    # Process each message in the list\n",
    "    for message in example['messages']:\n",
    "        if message.get('role') == 'user':\n",
    "            # Extract the question from user message\n",
    "            question = message.get('content', '').strip()\n",
    "        \n",
    "        elif message.get('role') == 'assistant':\n",
    "            # Extract the full response from assistant\n",
    "            response = message.get('content', '')\n",
    "            \n",
    "            # Split into reasoning (between <think> and </think>) and output (after </think>)\n",
    "            reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "            \n",
    "            # Output is everything after </think>, if it exists\n",
    "            output_start = response.find('</think>')\n",
    "            if output_start != -1:\n",
    "                output = response[output_start + 8:].strip()  # 8 is length of </think>\n",
    "            else:\n",
    "                output = response.strip()  # If no </think>, take the whole response\n",
    "            \n",
    "            # Clean up any residual tags (in case of malformed input)\n",
    "            reasoning = re.sub(r'<think>|</think>', '', reasoning).strip()\n",
    "            output = re.sub(r'<think>|</think>', '', output).strip()\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': output,\n",
    "        'reasoning': reasoning,\n",
    "        'combine': \"Question:\\n\" + question + \"\\Answer:\\n\" + output + \"\\Sure, this is my thinking process.\\n\"\n",
    "    }\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "dataset_long_fr = dataset_long_fr.map(process_messages)\n",
    "dataset_long_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8720f-9c8e-4ac1-aa1c-689c28363e51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_long_fr.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9166a-3f56-4e07-abf1-ca62990673b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_long_fr_json = process_generations_dataset_combine('fr-long.jsonl', 'reasoning')\n",
    "dataset_long_fr_json =  dataset_long_fr_json.remove_columns(\"combine\")\n",
    "# Print results\n",
    "print(dataset_long_fr_json)\n",
    "dataset_long_fr_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95594a-a111-4368-8a44-c7d58d45cc8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_long_fr_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd92e1-7981-448a-abad-f0c33aa66f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_long_fr_openai_messages = convert_to_openai_messages(\n",
    "    dataset_long_fr,\n",
    "    prompt_col=\"question\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"answer\",\n",
    "    source=\"fr-long\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_long_fr_openai_messages)\n",
    "dataset_long_fr_openai_messages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668923f-0d1f-41e7-bd5c-fcf9d78faa73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fr-qwen-single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25374c4-ac57-48bb-a679-ca5e4d0db67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_qwen_single_fr = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-qwen-single\", split='train')\n",
    "dataset_qwen_single_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23df694-be53-493e-ba7a-798aff8c5382",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    example['combine'] = f\"Question:\\n{example['instruction']}\\nAnswer:\\n{example['response']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_qwen_single_fr = dataset_qwen_single_fr.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_qwen_single_fr[0]['combine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481ab52-f99a-4590-9fe9-86b746812d9b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_qwen_single_fr.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-qwen-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2099b-8adc-44fc-bc44-f37783acf777",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_qwen_single_fr_json = process_generations_dataset_combine('fr-qwen-single.jsonl', 'reasoning')\n",
    "dataset_qwen_single_fr_json = dataset_qwen_single_fr_json.remove_columns(\"combine\")\n",
    "# Print results\n",
    "print(dataset_qwen_single_fr_json)\n",
    "dataset_qwen_single_fr_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203aa0a3-5371-465a-83ca-6796a1419f64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_qwen_single_fr_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-qwen-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c3d78-c8ae-4164-b14a-5de78b5fa70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_qwen_single_fr_openai_messages = convert_to_openai_messages(\n",
    "    dataset_qwen_single_fr,\n",
    "    prompt_col=\"instruction\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"response\",\n",
    "    source=\"fr-qwen-single\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_qwen_single_fr_openai_messages)\n",
    "dataset_qwen_single_fr_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c315e-94ef-4364-a507-63a7615e15af",
   "metadata": {},
   "source": [
    "## Fr-mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2244e3-6e54-4c4c-a1d9-9d2097eff670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_mt_fr = load_dataset(\"HoangHa/Pensez-v0.1\", \"fr-mt\", split='train')\n",
    "dataset_mt_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574b0d2-0a33-4eae-a5f2-0bb468f5471a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    example['combine_1'] = f\"Question:\\n{example['instruction']}\\nAnswer:\\n{example['response']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_mt_fr_1 = dataset_mt_fr.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_mt_fr_1[0]['combine_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0f0b6-3516-414a-8699-11bd1825d631",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_mt_fr_1.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a772a6b-11f9-4cd2-bc29-4fa11b8788bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_fr_1_json = process_generations_dataset_combine('fr-mt1.jsonl', 'reasoning_1')\n",
    "dataset_mt_fr_1_json = dataset_mt_fr_1_json.remove_columns(\"combine_1\")\n",
    "# Print results\n",
    "print(dataset_mt_fr_1_json)\n",
    "dataset_mt_fr_1_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ef46d-c365-4208-ae24-5966d1399a6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    example['combine_2'] = f\"Question:\\n{example['instruction']}\\n{example['reasoning_1']}\\nAnswer:\\n{example['response']}\\nQuestion:\\n{example['instruction_2']}\\nAnswer:\\n{example['response_2']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_mt_fr_2 = dataset_mt_fr_1_json.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_mt_fr_2[0]['combine_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081689b5-f2c0-40fd-bafc-5ef7520f9795",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_mt_fr_2.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d207f48-c76e-4935-875c-81530030329f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_fr_2_json = process_generations_dataset_combine('fr-mt2.jsonl', 'reasoning_2')\n",
    "dataset_mt_fr_2_json = dataset_mt_fr_2_json.remove_columns(\"combine_2\")\n",
    "# Print results\n",
    "print(dataset_mt_fr_2_json)\n",
    "dataset_mt_fr_2_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8661e6c-22f4-4a33-9fe1-6889f4b3442f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_mt_fr_2_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bbd42-182f-40a5-bed5-ab69756de8f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_mt_fr_2_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"fr-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b94f0-8749-4446-9aa5-038819f25783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_openai_messages_multiturn(\n",
    "    dataset,\n",
    "    source_label=\"fr-mt\"\n",
    "):\n",
    "    def format_row(row):\n",
    "        messages = []\n",
    "        \n",
    "        # First turn\n",
    "        if \"instruction\" in row and row[\"instruction\"]:\n",
    "            messages.append({\"role\": \"user\", \"content\": row[\"instruction\"]})\n",
    "            \n",
    "            # First assistant response with reasoning if available\n",
    "            if \"response\" in row:\n",
    "                content = \"\"\n",
    "                if \"reasoning_1\" in row and row[\"reasoning_1\"]:\n",
    "                    content += f\"<think>\\n{row['reasoning_1']}\\n</think>\\n\\n\"\n",
    "                content += f\"{row['response']}\"\n",
    "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        \n",
    "        # Second turn\n",
    "        if \"instruction_2\" in row and row[\"instruction_2\"]:\n",
    "            messages.append({\"role\": \"user\", \"content\": row[\"instruction_2\"]})\n",
    "            \n",
    "            # Second assistant response with reasoning if available\n",
    "            if \"response_2\" in row:\n",
    "                content = \"\"\n",
    "                if \"reasoning_2\" in row and row[\"reasoning_2\"]:\n",
    "                    content += f\"<think>\\n{row['reasoning_2']}\\n</think>\\n\\n\"\n",
    "                content += f\"{row['response_2']}\"\n",
    "                messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        \n",
    "        return {\"messages\": messages, \"source\": source_label}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Example usage:\n",
    "dataset_mt_fr_openai_messages = convert_to_openai_messages_multiturn(\n",
    "    dataset_mt_fr,\n",
    "    source_label=\"fr-mt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b382e63-99f9-4894-ac1f-b5ff138b549d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_mt_fr_openai_messages[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ce401-a96e-493a-92d6-098f3890a379",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64d32e-301b-48db-a27e-05604274dc06",
   "metadata": {},
   "source": [
    "## En-limo (Gen output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f0982-4c4f-4907-8d1a-688d8286b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_limo_en = load_dataset(\"HoangHa/Pensez-v0.1\", \"en-limo\", split='train')\n",
    "dataset_limo_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f45ed8-2708-4082-ace8-2f4cc7387097",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_limo_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab7bbf-71df-4c15-8ad5-740d17999c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    if example['question'] is None or example['solution'] is None:\n",
    "        example['combine'] = None\n",
    "    else:\n",
    "        example['combine'] = f\"Question:\\n{example['question']}\\Thinking process:\\n{example['solution']}\\nSure, this is my the conclusion.\\n\"\n",
    "    return example\n",
    "# Apply the transformation to create the new column\n",
    "dataset_limo_en = dataset_limo_en.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_limo_en[0]['combine'])\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(dataset, model_path=\"HoangHa/Pensez-v0.1-init\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def tokenize_and_count(example):\n",
    "        text = example[\"question\"] + example[\"solution\"]\n",
    "        return {\"token_count\": len(tokenizer(text).input_ids)}\n",
    "    \n",
    "    return dataset.map(tokenize_and_count)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_with_token_counts = count_tokens(dataset_limo_en)\n",
    "print(dataset_with_token_counts)\n",
    "\n",
    "max(dataset_with_token_counts['token_count'])\n",
    "\n",
    "dataset_with_token_counts.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-limo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f378cb-4b40-4cf5-9fa0-43ac932f753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-limo\" \\\n",
    "  --output-file \"en-limo.jsonl\" \\\n",
    "  --prompt-column \"combine\" \\\n",
    "  --uuid-column \"combine\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dd02f-0128-4b4c-a50a-fbde154a7761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_limo_en_json = process_generations_dataset_combine('en-limo-new.jsonl', 'output')\n",
    "dataset_limo_en_json = dataset_limo_en_json.remove_columns(\"combine\")\n",
    "# Print results\n",
    "print(dataset_limo_en_json)\n",
    "dataset_limo_en_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9824aa4-bf17-44fa-80de-1ce0080807cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_limo_en_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-limo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb65249-6baf-4048-ace2-bb4ebb434a78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_limo_en_openai_messages = convert_to_openai_messages(\n",
    "    dataset_limo_en_json,\n",
    "    prompt_col=\"question\",\n",
    "    reasoning_col=\"solution\",\n",
    "    answer_col=\"output\",\n",
    "    source=\"en-limo\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_limo_en_openai_messages)\n",
    "dataset_limo_en_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7842da9-b6b1-43c7-ad25-c3b5925f5286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## En-long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbec37f-7204-48cf-8600-070205cbc229",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697da111-4358-4c0c-b8b3-0c9a3e863f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_long_en = load_dataset(\"HoangHa/Pensez-v0.1\", \"en-long\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2144f-f497-40d3-a8bd-ecbdf4170f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_long_en[0]['reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3501c4-a33e-438f-bcdd-c303ba450305",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def process_messages(example):\n",
    "    \"\"\"\n",
    "    Process the 'messages' field to extract question, output, and reasoning into new columns.\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the dataset containing 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with new columns 'question', 'output', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    reasoning = \"\"\n",
    "    combine = \"\"\n",
    "    \n",
    "    # Check if 'messages' exists and is a list\n",
    "    if 'messages' not in example or not isinstance(example['messages'], list):\n",
    "        return {'question': question, 'output': output, 'reasoning': reasoning}\n",
    "    \n",
    "    # Process each message in the list\n",
    "    for message in example['messages']:\n",
    "        if message.get('role') == 'user':\n",
    "            # Extract the question from user message\n",
    "            question = message.get('content', '').strip()\n",
    "        \n",
    "        elif message.get('role') == 'assistant':\n",
    "            # Extract the full response from assistant\n",
    "            response = message.get('content', '')\n",
    "            \n",
    "            # Split into reasoning (between <think> and </think>) and output (after </think>)\n",
    "            reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "            \n",
    "            # Output is everything after </think>, if it exists\n",
    "            output_start = response.find('</think>')\n",
    "            if output_start != -1:\n",
    "                output = response[output_start + 8:].strip()  # 8 is length of </think>\n",
    "            else:\n",
    "                output = response.strip()  # If no </think>, take the whole response\n",
    "            \n",
    "            # Clean up any residual tags (in case of malformed input)\n",
    "            reasoning = re.sub(r'<think>|</think>', '', reasoning).strip()\n",
    "            output = re.sub(r'<think>|</think>', '', output).strip()\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': output,\n",
    "        'reasoning': reasoning,\n",
    "        'combine': \"Question:\\n\" + question + \"\\Answer:\\n\" + output + \"\\Sure, this is my thinking process.\\n\"\n",
    "    }\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "dataset_long_en = dataset_long_en.map(process_messages)\n",
    "dataset_long_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eacf25-eaaa-44ce-a188-d413d7e76d89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_long_en.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9c2be-b786-47b2-bab2-abf960bac0b3",
   "metadata": {},
   "source": [
    "### WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097952b8-5d7e-474f-9c84-9918a1ae97bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-long\" \\\n",
    "  --output-file \"en-long.jsonl\" \\\n",
    "  --prompt-column \"combine\" \\\n",
    "  --uuid-column \"combine\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7a4df-7562-43d3-b2e2-670925808ccd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_long_en_json = process_generations_dataset_combine('en-long.jsonl', 'reasoning')\n",
    "dataset_long_en_json =  dataset_long_en_json.remove_columns(\"combine\")\n",
    "# Print results\n",
    "print(dataset_long_en_json)\n",
    "dataset_long_en_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee6b66-fbaf-41c4-afe3-d00ca762fb17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_long_en.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8017b-6155-4877-83ea-cb24b0f5ec00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_long_en_openai_messages = convert_to_openai_messages(\n",
    "    dataset_long_en,\n",
    "    prompt_col=\"question\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"answer\",\n",
    "    source=\"en-long\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_long_en_openai_messages)\n",
    "dataset_long_en_openai_messages[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a0ac32d-81c6-4850-8e04-03b85a30be3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## En-single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ce623-7395-4f90-b15d-befe9040b530",
   "metadata": {},
   "source": [
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bab7bf-5755-40d4-8e47-fb0312f1bd67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_single_en = load_dataset(\"HoangHa/Pensez-v0.1\", \"en-single\", split='train')\n",
    "dataset_single_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50060e11-4316-4935-8533-a451fc40312a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def process_messages(example):\n",
    "    \"\"\"\n",
    "    Process the 'messages' field to extract question, output, and reasoning into new columns.\n",
    "    \n",
    "    Args:\n",
    "        example: A single example from the dataset containing 'messages' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with new columns 'question', 'output', 'reasoning'\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    reasoning = \"\"\n",
    "    combine = \"\"\n",
    "    \n",
    "    # Check if 'messages' exists and is a list\n",
    "    if 'messages' not in example or not isinstance(example['messages'], list):\n",
    "        return {'question': question, 'output': output, 'reasoning': reasoning}\n",
    "    \n",
    "    # Process each message in the list\n",
    "    for message in example['messages']:\n",
    "        if message.get('role') == 'user':\n",
    "            # Extract the question from user message\n",
    "            question = message.get('content', '').strip()\n",
    "        \n",
    "        elif message.get('role') == 'assistant':\n",
    "            # Extract the full response from assistant\n",
    "            response = message.get('content', '')\n",
    "            \n",
    "            # Split into reasoning (between <think> and </think>) and output (after </think>)\n",
    "            reasoning_match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "            \n",
    "            # Output is everything after </think>, if it exists\n",
    "            output_start = response.find('</think>')\n",
    "            if output_start != -1:\n",
    "                output = response[output_start + 8:].strip()  # 8 is length of </think>\n",
    "            else:\n",
    "                output = response.strip()  # If no </think>, take the whole response\n",
    "            \n",
    "            # Clean up any residual tags (in case of malformed input)\n",
    "            reasoning = re.sub(r'<think>|</think>', '', reasoning).strip()\n",
    "            output = re.sub(r'<think>|</think>', '', output).strip()\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': output,\n",
    "        'reasoning': reasoning,\n",
    "        'combine': \"Question:\\n\" + question + \"\\Answer:\\n\" + output + \"\\Sure, this is my thinking process.\\n\"\n",
    "    }\n",
    "\n",
    "# Apply the processing function to the dataset\n",
    "dataset_single_en = dataset_single_en.map(process_messages)\n",
    "dataset_single_en\n",
    "\n",
    "\n",
    "dataset_single_en.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08984e-afb9-47d2-a431-0dc70decb4e6",
   "metadata": {},
   "source": [
    "### WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1befad-0b41-4dc8-8217-d1eedb26ccba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-single\" \\\n",
    "  --output-file \"en-single.jsonl\" \\\n",
    "  --prompt-column \"combine\" \\\n",
    "  --uuid-column \"combine\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e1602-f19c-47ae-814c-06dc20f243f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_single_en_json = process_generations_dataset_combine('en-single.jsonl', 'reasoning')\n",
    "dataset_single_en_json =  dataset_single_en_json.remove_columns(\"combine\")\n",
    "# Print results\n",
    "print(dataset_single_en_json)\n",
    "dataset_single_en_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce7817-5b54-4b73-8d28-9c9d5e42f0f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_single_en_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869d4f7-c324-4d4b-a812-d2c6317bbd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n<{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_single_en_openai_messages = convert_to_openai_messages(\n",
    "    dataset_single_en,\n",
    "    prompt_col=\"question\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"answer\",\n",
    "    source=\"en-single\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_single_en_openai_messages)\n",
    "dataset_single_en_openai_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e7d10-94fd-4d21-ba55-225bbc7e06ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179b7291-9662-4e10-8063-9410e3c7da2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## En-single-reasoning-correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc317a21-0699-46f7-82b9-c18b69276234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_single_reasoning_en = load_dataset(\"HoangHa/Pensez-v0.1\", \"en-single-reasoning-correct\", split='train')\n",
    "# dataset_single_reasoning_en = dataset_single_reasoning_en.rename_column(\"messages\", \"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859e7f9-8f7e-4bee-baab-3785b67e04e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_tokens(dataset, model_path=\"HoangHa/Pensez-v0.1-init\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    def tokenize_and_count(example):\n",
    "        text = example[\"prompt\"] + example[\"reasoning\"] + example[\"answer\"]\n",
    "        return {\"token_count\": len(tokenizer(text).input_ids)}\n",
    "    \n",
    "    return dataset.map(tokenize_and_count)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_with_token_counts = count_tokens(dataset_single_reasoning_en)\n",
    "print(dataset_with_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e56d39-b4d8-455a-918f-4fa0d2fe0e1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_with_token_counts.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-single-reasoning-correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49778aa-ce29-4c24-a9a4-1940cf32cf9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, prompt_col=\"prompt\", reasoning_col=\"reasoning\", answer_col=\"answer\", source=\"default-source\"):\n",
    "    def format_row(row):\n",
    "        assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": row[prompt_col]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_single_reasoning_en_openai_messages = convert_to_openai_messages(\n",
    "    dataset_single_reasoning_en,\n",
    "    prompt_col=\"prompt\",\n",
    "    reasoning_col=\"reasoning\",\n",
    "    answer_col=\"answer\",\n",
    "    source=\"en-single-reasoning-correct\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_single_reasoning_en_openai_messages)\n",
    "dataset_single_reasoning_en_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4f5a2-4ef6-4dcb-9f9d-72a591d3da95",
   "metadata": {},
   "source": [
    "## En-mt (gen think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e15894-a004-4dad-b05a-b2223728a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_en_mt = load_dataset(\"HoangHa/Pensez-v0.1\", \"en-mt\", split='train')\n",
    "dataset_en_mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68964d4-2034-4dd6-a861-d559d7a4c1c0",
   "metadata": {},
   "source": [
    "### Turn 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71572a-a59c-494f-ad88-10beea8a4730",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def add_combine_column(example):\n",
    "    if example['instruction_1'] is None or example['answer_1'] is None:\n",
    "        example['combine_1'] = None\n",
    "    else:\n",
    "        example['combine_1'] = f\"Question:\\n{example['instruction_1']}\\nAnswer:\\n{example['answer_1']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_en_mt_1 = dataset_en_mt.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_en_mt_1[0]['combine_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46c20b-15d4-47df-9a81-e48b01b7b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en_mt_1.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a2b00-79bf-4044-9a46-1f3eead55727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-mt\" \\\n",
    "  --output-file \"en-mt-1.jsonl\" \\\n",
    "  --prompt-column \"combine_1\" \\\n",
    "  --uuid-column \"combine_1\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d611784-4989-4e0b-9ea8-79a78df5deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_en_1_json = process_generations_dataset_combine('en-mt-1.jsonl', 'reasoning_1')\n",
    "dataset_mt_en_1_json = dataset_mt_en_1_json.remove_columns(\"combine_1\")\n",
    "# Print results\n",
    "print(dataset_mt_en_1_json)\n",
    "dataset_mt_en_1_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5057e-b43b-47c9-b172-ce63112168b7",
   "metadata": {},
   "source": [
    "### Turn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ce347-2544-4f79-8c75-37e4d59196d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combine_column(example):\n",
    "    if any(example[key] is None for key in ['instruction_1', 'reasoning_1', 'answer_1', 'instruction_2', 'answer_2']):\n",
    "        example['combine_2'] = None\n",
    "    else:\n",
    "        example['combine_2'] = f\"Question:\\n{example['instruction_1']}\\nAnswer:\\n{example['reasoning_1']}\\n{example['answer_1']}\\nQuestion:\\n{example['instruction_2']}\\nAnswer:\\n{example['answer_2']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_mt_fr_2 = dataset_mt_fr_1_json.map(add_combine_column)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_mt_fr_2[0]['combine_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bace10-a747-4420-94f4-b965929c0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mt_fr_2.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0d93b-59b9-435a-af68-60a9ec3d242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-mt\" \\\n",
    "  --output-file \"en-mt-2.jsonl\" \\\n",
    "  --prompt-column \"combine_2\" \\\n",
    "  --uuid-column \"combine_2\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e3861-f5da-46f6-bf0e-5b005e44bf8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_en_2_json = process_generations_dataset_combine('en-mt-2.jsonl', 'reasoning_2')\n",
    "dataset_mt_en_2_json = dataset_mt_en_2_json.remove_columns(\"combine_2\")\n",
    "# Print results\n",
    "print(dataset_mt_en_2_json)\n",
    "dataset_mt_en_2_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ceb352-6327-4f14-8de0-6f522439d908",
   "metadata": {},
   "source": [
    "### Turn 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352625a-ccd5-4b5f-98e7-8b39592b5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combine_column_3(example):\n",
    "    if any(example[key] is None for key in ['instruction_1', 'reasoning_1', 'answer_1',\n",
    "                                            'instruction_2', 'reasoning_2', 'answer_2',\n",
    "                                            'instruction_3', 'answer_3']):\n",
    "        example['combine_3'] = None\n",
    "    else:\n",
    "        example['combine_3'] = f\"Question:\\n{example['instruction_1']}\\nAnswer:\\n{example['reasoning_1']}\\n{example['answer_1']}\\nQuestion:\\n{example['instruction_2']}\\nAnswer:\\n{example['reasoning_2']}\\n{example['answer_2']}\\nQuestion:\\n{example['instruction_3']}\\nAnswer:\\n{example['answer_3']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_mt_en_3 = dataset_mt_en_2_json.map(add_combine_column_3)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_mt_en_3[0]['combine_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aefbc2-5d8b-4d5a-bbba-8c56eb0e1e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_mt_en_3[50]['combine_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271062b-fd34-4d69-8b9f-c813d80bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mt_en_3.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4ac86-7bb6-4d21-90e9-e395668aac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-mt\" \\\n",
    "  --output-file \"en-mt-3.jsonl\" \\\n",
    "  --prompt-column \"combine_3\" \\\n",
    "  --uuid-column \"combine_3\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7805aef-3586-454e-adbc-0bd9a64ab0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"OpenMedical/m1-stage1-medical-reasoning\" \\\n",
    "  --dataset-sub \"r1-32b\" \\\n",
    "  --output-file \"question.jsonl\" \\\n",
    "  --prompt-column \"question\" \\\n",
    "  --uuid-column \"question\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 2048 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b38b35-0a58-4ee3-8ef9-506d9b0e8822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_en_3_json = process_generations_dataset_combine('en-mt-3.jsonl', 'reasoning_3')\n",
    "dataset_mt_en_3_json = dataset_mt_en_3_json.remove_columns(\"combine_3\")\n",
    "# Print results\n",
    "print(dataset_mt_en_3_json)\n",
    "dataset_mt_en_3_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76af1e1-a801-42c2-98a1-51a465ffba26",
   "metadata": {},
   "source": [
    "### Turn 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627fa2b-8e3c-4a5c-9cd8-eec813f89aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combine_column_4(example):\n",
    "    if any(example[key] is None for key in ['instruction_1', 'reasoning_1', 'answer_1',\n",
    "                                            'instruction_2', 'reasoning_2', 'answer_2',\n",
    "                                            'instruction_3', 'reasoning_3', 'answer_3',\n",
    "                                            'instruction_4', 'answer_4']):\n",
    "        example['combine_4'] = None\n",
    "    else:\n",
    "        example['combine_4'] = f\"Question:\\n{example['instruction_1']}\\nAnswer:\\n{example['reasoning_1']}\\n{example['answer_1']}\\nQuestion:\\n{example['instruction_2']}\\nAnswer:\\n{example['reasoning_2']}\\n{example['answer_2']}\\nQuestion:\\n{example['instruction_3']}\\nAnswer:\\n{example['reasoning_3']}\\n{example['answer_3']}\\nQuestion:\\n{example['instruction_4']}\\nAnswer:\\n{example['answer_4']}\\nSure, this is my thinking process.\\n\"\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to create the new column\n",
    "dataset_mt_en_4 = dataset_mt_en_3_json.map(add_combine_column_4)\n",
    "\n",
    "# Check the result\n",
    "print(dataset_mt_en_4[0]['combine_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcac6d-3829-4b04-a2c0-3a6a031fd812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_mt_en_4[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f3ce3-b877-4d89-ba75-636609e2f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mt_en_4_json.push_to_hub(\"HoangHa/Pensez-v0.1\", \"en-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62999d-b9f5-4d74-89ed-468642d889d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate + remember to change system prompt\n",
    "python sglang.py \\\n",
    "  --dataset-name \"HoangHa/Pensez-v0.1\" \\\n",
    "  --dataset-sub \"en-mt\" \\\n",
    "  --output-file \"en-mt-4.jsonl\" \\\n",
    "  --prompt-column \"combine_4\" \\\n",
    "  --uuid-column \"combine_4\" \\\n",
    "  --api-addr \"127.0.0.1:30000\" \\\n",
    "  --num-generations 1 \\\n",
    "  --max-tokens 16384 \\\n",
    "  --max-concurrent 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703722c5-e7b1-42b6-b12d-6c9010929bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_generations_dataset_combine(file_path, new_col):\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    dataset = dataset.map(lambda x: {new_col: x['generations'][0] if x['generations'] else ''})\n",
    "    dataset = dataset.remove_columns(\"generations\")\n",
    "    return dataset\n",
    "\n",
    "# Process both datasets\n",
    "dataset_mt_en_4_json = process_generations_dataset_combine('en-mt-4.jsonl', 'reasoning_4')\n",
    "dataset_mt_en_4_json = dataset_mt_en_4_json.remove_columns(\"combine_4\")\n",
    "# Print results\n",
    "print(dataset_mt_en_4_json)\n",
    "# dataset_mt_en_4_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e429ad-f6b2-409c-860f-df66786c2960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_openai_messages(dataset, base_instruction_col=\"instruction\", base_reasoning_col=\"reasoning\", base_answer_col=\"answer\", source=\"default-source\", max_turns=4):\n",
    "    def format_row(row):\n",
    "        messages = []\n",
    "        \n",
    "        # Function to check if a turn is complete\n",
    "        def has_complete_turn(turn_num):\n",
    "            # Construct column names for this turn\n",
    "            instruction_col = f\"{base_instruction_col}_{turn_num}\"\n",
    "            reasoning_col = f\"{base_reasoning_col}_{turn_num}\"\n",
    "            answer_col = f\"{base_answer_col}_{turn_num}\"\n",
    "            \n",
    "            # Check if all columns exist and have non-empty values\n",
    "            return all(col in row.keys() for col in [instruction_col, reasoning_col, answer_col]) and \\\n",
    "                   all(row[col] is not None and row[col] != \"\" for col in [instruction_col, reasoning_col, answer_col])\n",
    "        \n",
    "        # Iterate through all possible turns\n",
    "        for turn in range(1, max_turns + 1):\n",
    "            if has_complete_turn(turn):\n",
    "                # Get column names for this turn\n",
    "                instruction_col = f\"{base_instruction_col}_{turn}\"\n",
    "                reasoning_col = f\"{base_reasoning_col}_{turn}\"\n",
    "                answer_col = f\"{base_answer_col}_{turn}\"\n",
    "                \n",
    "                # Add user message\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": row[instruction_col]\n",
    "                })\n",
    "                \n",
    "                # Add assistant message with reasoning and answer\n",
    "                assistant_content = f\"<think>\\n{row[reasoning_col]}\\n</think>\\n\\n{row[answer_col]}\"\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_content\n",
    "                })\n",
    "                \n",
    "        return {\"messages\": messages, \"source\": source}\n",
    "\n",
    "    return dataset.map(format_row).select_columns([\"messages\", \"source\"])\n",
    "\n",
    "# Convert dataset to OpenAI message format with a source label\n",
    "dataset_mt_en_4_openai_messages = convert_to_openai_messages(\n",
    "    dataset_mt_en_4_json,\n",
    "    base_instruction_col=\"instruction\",\n",
    "    base_reasoning_col=\"reasoning\",\n",
    "    base_answer_col=\"answer\",\n",
    "    source=\"mt-en-4\",\n",
    "    max_turns=4  # Since your dataset has 4 turns\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(dataset_mt_en_4_openai_messages)\n",
    "dataset_mt_en_4_openai_messages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103509ea-09a5-4d56-a00c-222638accd6c",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1dec3-a839-4c59-9f63-bfe8db17b9af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset = concatenate_datasets([#fr\n",
    "                                dataset_fr_s1_openai_messages,\n",
    "                                dataset_fr_dolphin_openai_messages,\n",
    "                                dataset_fr_openmath_openai_messages,\n",
    "                                dataset_single_fr_openai_messages,\n",
    "                                dataset_single_llama_fr_openai_messages,\n",
    "                                dataset_long_fr_openai_messages,\n",
    "                                dataset_qwen_single_fr_openai_messages,\n",
    "                                dataset_mt_fr_openai_messages,\n",
    "                                # en\n",
    "                                dataset_long_en_openai_messages,\n",
    "                                dataset_single_reasoning_en_openai_messages,\n",
    "                                dataset_single_en_openai_messages,\n",
    "                                dataset_limo_en_openai_messages,\n",
    "                                dataset_mt_en_4_openai_messages\n",
    "                               ])\n",
    "print(dataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d959649f-94f5-4014-b0f2-e220a80b7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "model_path =  \"HoangHa/Pensez-v0.1-init\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def count_tokens(example, tokenizer):\n",
    "    \"\"\"Count tokens in messages that have a content field\"\"\"\n",
    "    total_tokens = 0\n",
    "    for message in example['messages']:\n",
    "        tokens = tokenizer(message['content'], return_tensors=\"pt\", truncation=False)\n",
    "        total_tokens += len(tokens['input_ids'][0])\n",
    "    example['token_count'] = total_tokens\n",
    "    return example\n",
    "    \n",
    "dataset = dataset.map(\n",
    "    lambda example: count_tokens(example, tokenizer), \n",
    "    num_proc=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab10bfa-11e0-4636-a5cb-e185b2cea1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "token_lengths = [sample[\"token_count\"] for sample in dataset]\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(token_lengths, bins=20, kde=True)\n",
    "plt.xlabel(\"Number of Messages per Conversation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Messages per Conversation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55504a-8872-4487-83da-29d6c76e079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of examples where token_count exceeds 16384\n",
    "exceeding_indices = [i for i, example in enumerate(dataset) if example['token_count'] > 16384]\n",
    "\n",
    "print(\"Indices with token_count > 16384:\", exceeding_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "654ac1be-0411-4b98-ad17-c48fb33cdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_assistant_response = {'content': \"<think>\\nLet's break down this problem step-by-step, combining physics principles and calculations. **1. Understanding the Physics Principles** * **De Broglie Wavelength:** The wave nature of particles is crucial here. The de Broglie wavelength (λ) is given by λ = h/p, where h is Planck's constant and p is the momentum of the particle. * **Momentum and Kinetic Energy:** For non-relativistic particles, kinetic energy (KE) = 1/2 * mv², and momentum p = mv. Therefore, KE = p²/2m. * **Average Kinetic Energy and Temperature:** For a gas in thermal equilibrium, the average kinetic energy of the particles is proportional to the temperature: KE_avg = (f/2)kT, where f is the number of degrees of freedom (3 for translational motion in this case), k is the Boltzmann constant, and T is the temperature. * **Average Speed:** The particles selected by the filter have a speed close to the average speed. We can relate this average speed to the temperature. * **Single-Slit Diffraction:** The phenomenon of bright and dark regions on the screen indicates single-slit diffraction. The condition for constructive interference (bright regions) in single-slit diffraction is given by: d sin θ = (m + 1/2)λ, where d is the slit width, θ is the angle of the bright fringe, and m is the order of the fringe (m = 0 for the first bright region, m = 1 for the second, etc.). **2. Analyzing the First Scenario (Mass m1, Temperature T1)** * **Average Speed:** The average kinetic energy is (3/2)kT1 = (1/2)m1 * v1_avg². Therefore, v1_avg = sqrt(3kT1/m1). * **De Broglie Wavelength:** The de Broglie wavelength of the selected particles is λ1 = h / (m1 * v1_avg) = h / (m1 * sqrt(3kT1/m1)) = h / sqrt(3km1T1). * **First Bright Region:** For the first bright region (m=0), the condition for constructive interference is: d sin θ1 = (0 + 1/2)λ1 = λ1/2. * **Substituting λ1:** d sin θ1 = h / (2 * sqrt(3km1T1)). **3. Analyzing the Second Scenario (Mass m2, Temperature T2)** * **Average Speed:** The average kinetic energy is (3/2)kT2 = (1/2)m2 * v2_avg². Therefore, v2_avg = sqrt(3kT2/m2). * **De Broglie Wavelength:** The de Broglie wavelength of the selected particles is λ2 = h / (m2 * v2_avg) = h / (m2 * sqrt(3kT2/m2)) = h / sqrt(3km2T2). * **Second Bright Region:** For the second bright region (m=1), the condition for constructive interference is: d sin θ2 = (1 + 1/2)λ2 = 3λ2/2. * **Substituting λ2:** d sin θ2 = 3h / (2 * sqrt(3km2T2)). **4. Setting up the Equations and Solving for T1/T2** We have two equations: Equation 1: d sin θ1 = h / (2 * sqrt(3km1T1)) Equation 2: d sin θ2 = 3h / (2 * sqrt(3km2T2)) We are given d = 2.5 micrometers, θ1 = 15 degrees, θ2 = 20 degrees, and m2 = 2m1. We want to find T1/T2. Divide Equation 1 by Equation 2: (d sin θ1) / (d sin θ2) = [h / (2 * sqrt(3km1T1))] / [3h / (2 * sqrt(3km2T2))] Simplify: sin θ1 / sin θ2 = (1/3) * sqrt(3km2T2) / sqrt(3km1T1) sin θ1 / sin θ2 = (1/3) * sqrt(m2T2 / (m1T1)) Substitute m2 = 2m1: sin θ1 / sin θ2 = (1/3) * sqrt(2m1T2 / (m1T1)) sin θ1 / sin θ2 = (1/3) * sqrt(2 * (T2/T1)) Now, plug in the values for θ1 and θ2: sin(15°) / sin(20°) = (1/3) * sqrt(2 * (T2/T1)) Calculate the sine values: sin(15°) ≈ 0.2588 sin(20°) ≈ 0.3420 0.2588 / 0.3420 = (1/3) * sqrt(2 * (T2/T1)) 0.7567 ≈ (1/3) * sqrt(2 * (T2/T1)) Multiply by 3: 2.2701 ≈ sqrt(2 * (T2/T1)) Square both sides: (2.2701)² ≈ 2 * (T2/T1) 5.153 ≈ 2 * (T2/T1) Solve for T1/T2: T1/T2 ≈ 2 / 5.153 T1/T2 ≈ 0.388 Something is wrong, I must have confused the bright fringe order. Let's revisit the bright fringe condition. **Revisiting the Bright Fringe Condition** The condition for *constructive* interference in single-slit diffraction is actually more complex and involves minima. However, the problem statement refers to \\\"bright regions\\\", which are often approximated using a simplified approach related to the angles of the minima. A better approach might be to consider the path difference for the *first* bright fringe away from the central maximum. Alternatively, we might be interpreting \\\"first bright one\\\" as the first *principal maximum*, which occurs at the center (m=0), and the next bright regions correspond to the secondary maxima. However, in single slit diffraction, the secondary maxima are much weaker than the central maximum. The wording \\\"a series of bright and dark regions\\\" strongly suggests diffraction minima. Let's consider the condition for the *minima* in single-slit diffraction, as the bright fringes lie between them. The condition for the *m-th minimum* is d sin θ = mλ. The first bright region is roughly halfway between the 0th and 1st minimum. **Revised Approach using Minima** * **First Scenario (m1, T1):** * λ1 = h / sqrt(3km1T1) * The first bright region is roughly where the path difference is λ/2. This occurs approximately where the angle is such that it's halfway to the first minimum. The angle for the first minimum is d sin θ_min1 = 1 * λ1. The angle for the first bright fringe is more complex to precisely determine, but let's assume the problem uses the simplified condition for constructive interference, d sin θ = (m + 1/2)λ. Let's stick with the constructive interference condition as initially interpreted, because the answer choices suggest a straightforward calculation is intended. The wording \\\"first bright one (closest to the center of the slit)\\\" implies m=0. \\\"the second bright region\\\" in the second scenario implies m=1. **Double Checking the Calculations** sin(15°) / sin(20°) = (1/3) * sqrt(2 * (T2/T1)) 0.7567 = (1/3) * sqrt(2 * (T2/T1)) 2.2701 = sqrt(2 * (T2/T1)) 5.153 = 2 * (T2/T1) T2/T1 = 5.153 / 2 = 2.5765 T1/T2 = 1 / 2.5765 = 0.388 There must be a mistake in the interpretation of the bright fringe order. The \\\"first bright one\\\" usually refers to the m=1 bright fringe in double-slit interference, but in single-slit diffraction, it refers to the regions between the minima. Let's consider the angles given are for the *first diffraction maximum* away from the central maximum. The condition for this is approximately given by the constructive interference formula. **Re-evaluating the Bright Fringe Order** In single-slit diffraction, the central maximum is at θ = 0. The subsequent bright fringes are located approximately halfway between the minima. * **Minima:** d sin θ = mλ (m = 1, 2, 3, ...) * **Bright fringes (approximate):** d sin θ ≈ (m + 1/2)λ For the first bright region (closest to the center, excluding the central max), m = 1. For the second bright region, m = 2. **Scenario 1 (First bright region):** d sin θ1 = (1 + 1/2)λ1 = (3/2)λ1 d sin 15° = (3/2) * h / sqrt(3km1T1) **Scenario 2 (Second bright region):** d sin θ2 = (2 + 1/2)λ2 = (5/2)λ2 d sin 20° = (5/2) * h / sqrt(3km2T2) Now, divide the two equations: (d sin 15°) / (d sin 20°) = [(3/2) * h / sqrt(3km1T1)] / [(5/2) * h / sqrt(3km2T2)] sin 15° / sin 20° = (3/5) * sqrt(3km2T2) / sqrt(3km1T1) sin 15° / sin 20° = (3/5) * sqrt(m2T2 / (m1T1)) Substitute m2 = 2m1: sin 15° / sin 20° = (3/5) * sqrt(2m1T2 / (m1T1)) sin 15° / sin 20° = (3/5) * sqrt(2 * (T2/T1)) 0.7567 = (3/5) * sqrt(2 * (T2/T1)) Multiply by 5/3: 0.7567 * (5/3) = sqrt(2 * (T2/T1)) 1.261 = sqrt(2 * (T2/T1)) Square both sides: (1.261)² = 2 * (T2/T1) 1.59 = 2 * (T2/T1) T2/T1 = 1.59 / 2 = 0.795 T1/T2 = 1 / 0.795 = 1.257 This is close to option (B) 1.3. Let's verify the assumption about the bright fringe order. The \\\"first bright one closest to the center\\\" is typically the first one *outside* the central maximum. Consider the diffraction pattern intensity: the central maximum is the brightest, and the subsequent maxima are less intense. The positions are roughly halfway between the minima. If the \\\"first bright one\\\" indeed refers to m=0 in the constructive interference formula: Scenario 1: d sin 15° = (1/2)λ1 Scenario 2: d sin 20° = (3/2)λ2 Dividing: sin 15° / sin 20° = (1/3) * sqrt(m2T2 / (m1T1)) 0.7567 = (1/3) * sqrt(2 * T2/T1) 2.2701 = sqrt(2 * T2/T1) 5.153 = 2 * T2/T1 T2/T1 = 2.5765 T1/T2 = 0.388 (This doesn't match any options.) Let's assume the problem implicitly refers to the bright fringes as described by the constructive interference formula. The wording is slightly ambiguous for single-slit diffraction, where the central maximum is the m=0 case. If \\\"first bright one\\\" means the first *secondary* maximum, it corresponds to roughly m=1 in the constructive interference approximation. If the problem intends the standard single-slit diffraction minima approach, the bright fringes are between them. The first bright fringe is roughly at the angle where sin θ = 3λ/2d. Let's stick with the constructive interference interpretation as it leads to a plausible answer. The phrase \\\"first bright one (closest to the center of the slit)\\\" likely implies the first order bright fringe beyond the central maximum. With the assumption that the bright fringes follow d sin θ = (m + 1/2)λ: First bright region: m=0 (This would be the central maximum, but the problem implies it's the *first one* observed as you move away from the center.) Let's interpret \\\"first bright one\\\" as the first side maximum. If the bright fringes are exactly halfway between the minima: First bright fringe position is related to the first minimum at d sin θ = λ. The bright fringe is around sin θ ≈ 3λ/2d. If the first bright one corresponds to the condition d sin θ = 3λ/2, and the second bright one to d sin θ = 5λ/2. Scenario 1: d sin 15° = 3λ1/2 Scenario 2: d sin 20° = 5λ2/2 sin 15° / sin 20° = (3λ1/2) / (5λ2/2) = 3λ1 / 5λ2 0.7567 = (3/5) * sqrt(m2T2 / (m1T1)) 0.7567 = (3/5) * sqrt(2 T2 / T1) 1.261 = sqrt(2 T2 / T1) 1.59 = 2 T2 / T1 T2 / T1 = 0.795 T1 / T2 = 1.257 This matches option (B) if rounded. The key is the interpretation of \\\"first bright one\\\". Given the answer choices, the constructive interference approximation seems to be what the question setters intended. Solution: The condition for constructive interference (bright regions) in single-slit diffraction can be approximated by: d sin θ = (m + 1/2)λ where d is the slit width, θ is the angle of the bright fringe, λ is the de Broglie wavelength, and m is the order of the fringe (m = 0 for the first bright region, m = 1 for the second, etc.). The de Broglie wavelength is given by λ = h/p = h/(mv), and the average kinetic energy is (3/2)kT = (1/2)mv², so v = sqrt(3kT/m). Thus, λ = h / (m * sqrt(3kT/m)) = h / sqrt(3mkT). For the first scenario (mass m1, temperature T1), the first bright region (m=0) is at θ1 = 15 degrees: d sin θ1 = (0 + 1/2)λ1 = (1/2) * h / sqrt(3km1T1) (Equation 1) For the second scenario (mass m2 = 2m1, temperature T2), the second bright region (m=1) is at θ2 = 20 degrees: d sin θ2 = (1 + 1/2)λ2 = (3/2) * h / sqrt(3km2T2) (Equation 2) Divide Equation 1 by Equation 2: (d sin θ1) / (d sin θ2) = [(1/2) * h / sqrt(3km1T1)] / [(3/2) * h / sqrt(3km2T2)] sin θ1 / sin θ2 = (1/3) * sqrt(3km2T2) / sqrt(3km1T1) sin θ1 / sin θ2 = (1/3) * sqrt(m2T2 / (m1T1)) Substitute the given values θ1 = 15°, θ2 = 20°, and m2 = 2m1: sin(15°) / sin(20°) = (1/3) * sqrt(2m1T2 / (m1T1)) 0.2588 / 0.3420 = (1/3) * sqrt(2 * (T2/T1)) 0.7567 ≈ (1/3) * sqrt(2 * (T2/T1)) 2.2701 ≈ sqrt(2 * (T2/T1)) Squaring both sides: 5.153 ≈ 2 * (T2/T1) T2/T1 ≈ 5.153 / 2 = 2.5765 T1/T2 ≈ 1 / 2.5765 ≈ 0.388 This interpretation assumed m=0 for the \\\"first bright one\\\". If \\\"first bright one\\\" refers to the first side maximum, then for the first scenario m=1, and for the second scenario the \\\"second bright region\\\" would be m=2. d sin θ1 = (1 + 1/2)λ1 = 3λ1/2 d sin θ2 = (2 + 1/2)λ2 = 5λ2/2 sin 15° / sin 20° = (3λ1/2) / (5λ2/2) = (3/5) * sqrt(m2T2 / (m1T1)) 0.7567 = (3/5) * sqrt(2 T2 / T1) 1.261 = sqrt(2 T2 / T1) 1.59 = 2 T2 / T1 T2 / T1 = 0.795 T1 / T2 = 1.258 Final Answer: The final answer is $\\\\boxed{1.3}$</think>\\\\n\\\\nThe problem involves understanding the interplay between De Broglie wavelength, single-slit diffraction, and the relationship between kinetic energy and temperature. The key is correctly interpreting the conditions for bright fringes in single-slit diffraction. By analyzing the two scenarios (mass m1, temperature T1, first bright fringe; and mass m2, temperature T2, second bright fringe), and using the approximation d sin θ = (m + 1/2)λ for the bright fringes (where 'm' is re-interpreted to mean the order number from the *central maximum*), we set up two equations. Given that m2 = 2m1, θ1 = 15 degrees, and θ2 = 20 degrees, by dividing these equations and solving for the ratio of temperatures, we interpret the 'first bright one' as m=1 and 'second bright region' as m=2. We find sin(15°)/sin(20°) = (3/5)sqrt(2T2/T1). Solving this gives T1/T2 ≈ 1.258, which is closest to 1.3.\", 'role': 'assistant'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dd32e-8b07-4c37-8fe6-03f4b4009696",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modified_dataset = dataset.map(lambda example, idx: {'messages': [example['messages'][0], {'content': new_assistant_response, 'role': 'assistant'}]} if idx == 60 else example, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd791d17-6d9c-44ac-b9bf-da28c37962eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b5f32-b0f9-4b8a-8296-fd3355400642",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"HoangHa/Pensez-v0.1-fr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
